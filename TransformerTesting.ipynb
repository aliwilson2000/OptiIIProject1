{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliwilson2000/OptiIIProject1/blob/main/TransformerTestingProcess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers for MNIST"
      ],
      "metadata": {
        "id": "By7SEzdt-rmd"
      },
      "id": "By7SEzdt-rmd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64eaf0cf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "id": "64eaf0cf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4y4M85AL8vY",
        "outputId": "f44a8edc-b2a1-4c96-f647-0a65e7ac8506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # mounting drive for easy read-write\n",
        "drive.mount('/content/drive')"
      ],
      "id": "x4y4M85AL8vY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT Model 1:"
      ],
      "metadata": {
        "id": "jFv8qPHS_Mha"
      },
      "id": "jFv8qPHS_Mha"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43f4253e"
      },
      "outputs": [],
      "source": [
        "# this is written as a tensorflow \"layer\".  it's just a vector the same size as the\n",
        "# output of the previous layer. the vector is initialized randomly, but we'll use\n",
        "# gradient descent to update the values in the vector\n",
        "#\n",
        "# it's purpose is to be appended to the beginning of the sequence of vectors fed into\n",
        "# the transformer.  then after the transformer runs on the whole data, we just grab\n",
        "# the resulting zero-th vector...the class token...and use that as the portfolio weights\n",
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls"
      ],
      "id": "43f4253e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56d0c131"
      },
      "outputs": [],
      "source": [
        "def build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes):\n",
        "    # n is number of rows of blocks\n",
        "    # m is number of cols of blocks\n",
        "    # block_size is number of pixels (with rgb) in each block\n",
        "\n",
        "    inp = tf.keras.layers.Input(shape=(n*m,block_size))\n",
        "    inp2 = tf.keras.layers.Input(shape=(n*m))\n",
        "    mid = tf.keras.layers.Dense(hidden_dim)(inp) # transform to vectors with different dimension\n",
        "    # the positional embeddings\n",
        "#     positions = tf.range(start=0, limit=n*m, delta=1)\n",
        "    emb = tf.keras.layers.Embedding(input_dim=n*m, output_dim=hidden_dim)(inp2)\n",
        "    # learned positional embedding for each of the n*m possible possitions\n",
        "    mid = mid + emb # for some reason, tf.keras.layers.Add causes an error, but + doesn't?\n",
        "    # create and append class token to beginning of all input vectors\n",
        "    token = ClassToken()(mid) # append class token to beginning of sequence\n",
        "    mid = tf.keras.layers.Concatenate(axis=1)([token, mid])\n",
        "\n",
        "    for l in range(num_layers): # how many Transformer Head layers are there?\n",
        "        ln  = tf.keras.layers.LayerNormalization()(mid) # normalize\n",
        "        mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,value_dim=key_dim)(ln,ln,ln)\n",
        "        # self attention!\n",
        "        add = tf.keras.layers.Add()([mid,mha]) # add and norm\n",
        "        ln  = tf.keras.layers.LayerNormalization()(add)\n",
        "        den = tf.keras.layers.Dense(mlp_dim,activation='gelu')(ln) # maybe should be relu...who knows...\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den) # regularization\n",
        "        den = tf.keras.layers.Dense(hidden_dim)(den) # back to the right dimensional space\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        mid = tf.keras.layers.Add()([den,add]) # add and norm again\n",
        "    ln = tf.keras.layers.LayerNormalization()(mid)\n",
        "    fl = ln[:,0,:] # just grab the class token for each image in batch\n",
        "    clas = tf.keras.layers.Dense(num_classes,activation='softmax')(fl)\n",
        "    # probability that the image is in each category\n",
        "    mod = tf.keras.models.Model([inp,inp2],clas)\n",
        "    mod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mod"
      ],
      "id": "56d0c131"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19ce4b9d",
        "outputId": "1a9dfecf-aaad-4d50-c27b-3af863970da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 16, 49)]             0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 16)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 16, 128)              6400      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 16, 128)              2048      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 16, 128)              0         ['dense[0][0]',               \n",
            " Lambda)                                                             'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " class_token (ClassToken)    (None, 1, 128)               128       ['tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 17, 128)              0         ['class_token[0][0]',         \n",
            "                                                                     'tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 17, 128)              256       ['concatenate[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 17, 128)              66048     ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]', \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 17, 128)              0         ['concatenate[0][0]',         \n",
            "                                                                     'multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 17, 128)              256       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 17, 128)              16512     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 17, 128)              0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 17, 128)              16512     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 17, 128)              0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 17, 128)              0         ['dropout_1[0][0]',           \n",
            "                                                                     'add[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 17, 128)              256       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 17, 128)              66048     ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 17, 128)              0         ['add_1[0][0]',               \n",
            "                                                                     'multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 17, 128)              256       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 17, 128)              16512     ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 17, 128)              0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 17, 128)              16512     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 17, 128)              0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 17, 128)              0         ['dropout_3[0][0]',           \n",
            "                                                                     'add_2[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 17, 128)              256       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 17, 128)              66048     ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 17, 128)              0         ['add_3[0][0]',               \n",
            "                                                                     'multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 17, 128)              256       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 17, 128)              16512     ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 17, 128)              0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 17, 128)              16512     ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 17, 128)              0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 17, 128)              0         ['dropout_5[0][0]',           \n",
            "                                                                     'add_4[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 17, 128)              256       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 17, 128)              66048     ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 17, 128)              0         ['add_5[0][0]',               \n",
            "                                                                     'multi_head_attention_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 17, 128)              256       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 17, 128)              16512     ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 17, 128)              0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 17, 128)              16512     ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 17, 128)              0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 17, 128)              0         ['dropout_7[0][0]',           \n",
            "                                                                     'add_6[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 17, 128)              256       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 17, 128)              66048     ['layer_normalization_8[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 17, 128)              0         ['add_7[0][0]',               \n",
            "                                                                     'multi_head_attention_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 17, 128)              256       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 17, 128)              16512     ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 17, 128)              0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 17, 128)              16512     ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 17, 128)              0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 17, 128)              0         ['dropout_9[0][0]',           \n",
            "                                                                     'add_8[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_10 (La  (None, 17, 128)              256       ['add_9[0][0]']               \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 17, 128)              66048     ['layer_normalization_10[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 17, 128)              0         ['add_9[0][0]',               \n",
            "                                                                     'multi_head_attention_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_11 (La  (None, 17, 128)              256       ['add_10[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 17, 128)              16512     ['layer_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 17, 128)              0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 17, 128)              16512     ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 17, 128)              0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 17, 128)              0         ['dropout_11[0][0]',          \n",
            "                                                                     'add_10[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_12 (La  (None, 17, 128)              256       ['add_11[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (Mu  (None, 17, 128)              66048     ['layer_normalization_12[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 17, 128)              0         ['add_11[0][0]',              \n",
            "                                                                     'multi_head_attention_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_13 (La  (None, 17, 128)              256       ['add_12[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 17, 128)              16512     ['layer_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 17, 128)              0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 17, 128)              16512     ['dropout_12[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 17, 128)              0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 17, 128)              0         ['dropout_13[0][0]',          \n",
            "                                                                     'add_12[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_14 (La  (None, 17, 128)              256       ['add_13[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (Mu  (None, 17, 128)              66048     ['layer_normalization_14[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 17, 128)              0         ['add_13[0][0]',              \n",
            "                                                                     'multi_head_attention_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_15 (La  (None, 17, 128)              256       ['add_14[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 17, 128)              16512     ['layer_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 17, 128)              0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 17, 128)              16512     ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)        (None, 17, 128)              0         ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 17, 128)              0         ['dropout_15[0][0]',          \n",
            "                                                                     'add_14[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_16 (La  (None, 17, 128)              256       ['add_15[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (Mu  (None, 17, 128)              66048     ['layer_normalization_16[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_16[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_16 (Add)                (None, 17, 128)              0         ['add_15[0][0]',              \n",
            "                                                                     'multi_head_attention_8[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_17 (La  (None, 17, 128)              256       ['add_16[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 17, 128)              16512     ['layer_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)        (None, 17, 128)              0         ['dense_17[0][0]']            \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 17, 128)              16512     ['dropout_16[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)        (None, 17, 128)              0         ['dense_18[0][0]']            \n",
            "                                                                                                  \n",
            " add_17 (Add)                (None, 17, 128)              0         ['dropout_17[0][0]',          \n",
            "                                                                     'add_16[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_18 (La  (None, 17, 128)              256       ['add_17[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (Mu  (None, 17, 128)              66048     ['layer_normalization_18[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_18[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_18 (Add)                (None, 17, 128)              0         ['add_17[0][0]',              \n",
            "                                                                     'multi_head_attention_9[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_19 (La  (None, 17, 128)              256       ['add_18[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 17, 128)              16512     ['layer_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)        (None, 17, 128)              0         ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 17, 128)              16512     ['dropout_18[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)        (None, 17, 128)              0         ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            " add_19 (Add)                (None, 17, 128)              0         ['dropout_19[0][0]',          \n",
            "                                                                     'add_18[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_20 (La  (None, 17, 128)              256       ['add_19[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (M  (None, 17, 128)              66048     ['layer_normalization_20[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_20[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_20 (Add)                (None, 17, 128)              0         ['add_19[0][0]',              \n",
            "                                                                     'multi_head_attention_10[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_21 (La  (None, 17, 128)              256       ['add_20[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, 17, 128)              16512     ['layer_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)        (None, 17, 128)              0         ['dense_21[0][0]']            \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 17, 128)              16512     ['dropout_20[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)        (None, 17, 128)              0         ['dense_22[0][0]']            \n",
            "                                                                                                  \n",
            " add_21 (Add)                (None, 17, 128)              0         ['dropout_21[0][0]',          \n",
            "                                                                     'add_20[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_22 (La  (None, 17, 128)              256       ['add_21[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (M  (None, 17, 128)              66048     ['layer_normalization_22[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_22[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_22 (Add)                (None, 17, 128)              0         ['add_21[0][0]',              \n",
            "                                                                     'multi_head_attention_11[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_23 (La  (None, 17, 128)              256       ['add_22[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, 17, 128)              16512     ['layer_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)        (None, 17, 128)              0         ['dense_23[0][0]']            \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, 17, 128)              16512     ['dropout_22[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)        (None, 17, 128)              0         ['dense_24[0][0]']            \n",
            "                                                                                                  \n",
            " add_23 (Add)                (None, 17, 128)              0         ['dropout_23[0][0]',          \n",
            "                                                                     'add_22[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_24 (La  (None, 17, 128)              256       ['add_23[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (M  (None, 17, 128)              66048     ['layer_normalization_24[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_24[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_24 (Add)                (None, 17, 128)              0         ['add_23[0][0]',              \n",
            "                                                                     'multi_head_attention_12[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_25 (La  (None, 17, 128)              256       ['add_24[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_25 (Dense)            (None, 17, 128)              16512     ['layer_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)        (None, 17, 128)              0         ['dense_25[0][0]']            \n",
            "                                                                                                  \n",
            " dense_26 (Dense)            (None, 17, 128)              16512     ['dropout_24[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)        (None, 17, 128)              0         ['dense_26[0][0]']            \n",
            "                                                                                                  \n",
            " add_25 (Add)                (None, 17, 128)              0         ['dropout_25[0][0]',          \n",
            "                                                                     'add_24[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_26 (La  (None, 17, 128)              256       ['add_25[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (M  (None, 17, 128)              66048     ['layer_normalization_26[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_26[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_26 (Add)                (None, 17, 128)              0         ['add_25[0][0]',              \n",
            "                                                                     'multi_head_attention_13[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_27 (La  (None, 17, 128)              256       ['add_26[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_27 (Dense)            (None, 17, 128)              16512     ['layer_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)        (None, 17, 128)              0         ['dense_27[0][0]']            \n",
            "                                                                                                  \n",
            " dense_28 (Dense)            (None, 17, 128)              16512     ['dropout_26[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)        (None, 17, 128)              0         ['dense_28[0][0]']            \n",
            "                                                                                                  \n",
            " add_27 (Add)                (None, 17, 128)              0         ['dropout_27[0][0]',          \n",
            "                                                                     'add_26[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_28 (La  (None, 17, 128)              256       ['add_27[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (M  (None, 17, 128)              66048     ['layer_normalization_28[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_28[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_28 (Add)                (None, 17, 128)              0         ['add_27[0][0]',              \n",
            "                                                                     'multi_head_attention_14[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_29 (La  (None, 17, 128)              256       ['add_28[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_29 (Dense)            (None, 17, 128)              16512     ['layer_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)        (None, 17, 128)              0         ['dense_29[0][0]']            \n",
            "                                                                                                  \n",
            " dense_30 (Dense)            (None, 17, 128)              16512     ['dropout_28[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)        (None, 17, 128)              0         ['dense_30[0][0]']            \n",
            "                                                                                                  \n",
            " add_29 (Add)                (None, 17, 128)              0         ['dropout_29[0][0]',          \n",
            "                                                                     'add_28[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_30 (La  (None, 17, 128)              256       ['add_29[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (M  (None, 17, 128)              66048     ['layer_normalization_30[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_30[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_30 (Add)                (None, 17, 128)              0         ['add_29[0][0]',              \n",
            "                                                                     'multi_head_attention_15[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_31 (La  (None, 17, 128)              256       ['add_30[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_31 (Dense)            (None, 17, 128)              16512     ['layer_normalization_31[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)        (None, 17, 128)              0         ['dense_31[0][0]']            \n",
            "                                                                                                  \n",
            " dense_32 (Dense)            (None, 17, 128)              16512     ['dropout_30[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)        (None, 17, 128)              0         ['dense_32[0][0]']            \n",
            "                                                                                                  \n",
            " add_31 (Add)                (None, 17, 128)              0         ['dropout_31[0][0]',          \n",
            "                                                                     'add_30[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_32 (La  (None, 17, 128)              256       ['add_31[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 128)                  0         ['layer_normalization_32[0][0]\n",
            " SlicingOpLambda)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_33 (Dense)            (None, 10)                   1290      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1603466 (6.12 MB)\n",
            "Trainable params: 1603466 (6.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "n = 4\n",
        "m = 4\n",
        "block_size = 49\n",
        "hidden_dim = 128\n",
        "num_layers = 16\n",
        "num_heads = 8\n",
        "key_dim = hidden_dim//num_heads\n",
        "mlp_dim = hidden_dim\n",
        "dropout_rate = 0.01\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "\n",
        "trans = build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
        "trans.summary()"
      ],
      "id": "19ce4b9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e1adb6a"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "ndata_train = x_train.shape[0]\n",
        "ndata_test = x_test.shape[0]"
      ],
      "id": "8e1adb6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fycdF0tzsFQ7"
      },
      "source": [
        "Preparing train and test data and flattening it to be fed to the transformer."
      ],
      "id": "fycdF0tzsFQ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7359ed66"
      },
      "outputs": [],
      "source": [
        "x_train_ravel = np.zeros((ndata_train,n*m,block_size))\n",
        "for img in range(ndata_train):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_train_ravel[img,ind,:] = x_train[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1\n"
      ],
      "id": "7359ed66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67e3829d"
      },
      "outputs": [],
      "source": [
        "x_test_ravel = np.zeros((ndata_test,n*m,block_size))\n",
        "for img in range(ndata_test):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_test_ravel[img,ind,:] = x_test[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1"
      ],
      "id": "67e3829d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a64dd97"
      },
      "outputs": [],
      "source": [
        "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
        "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
      ],
      "id": "7a64dd97"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJKa2tan1Dw4",
        "outputId": "3978010f-a29d-4b3b-b3fb-32cda922f1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "188/188 [==============================] - 66s 117ms/step - loss: 0.9813 - accuracy: 0.6681 - val_loss: 0.2427 - val_accuracy: 0.9226 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 0.1810 - accuracy: 0.9434 - val_loss: 0.1362 - val_accuracy: 0.9594 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 0.1067 - accuracy: 0.9666 - val_loss: 0.1089 - val_accuracy: 0.9674 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 0.0793 - accuracy: 0.9750 - val_loss: 0.0999 - val_accuracy: 0.9709 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 0.0671 - accuracy: 0.9782 - val_loss: 0.0979 - val_accuracy: 0.9718 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 0.0563 - accuracy: 0.9812 - val_loss: 0.0881 - val_accuracy: 0.9729 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 0.0478 - accuracy: 0.9841 - val_loss: 0.0889 - val_accuracy: 0.9750 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "188/188 [==============================] - 19s 104ms/step - loss: 0.0455 - accuracy: 0.9851 - val_loss: 0.0831 - val_accuracy: 0.9771 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.0826 - val_accuracy: 0.9766 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 0.0381 - accuracy: 0.9875 - val_loss: 0.0831 - val_accuracy: 0.9768 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 0.0273 - accuracy: 0.9906 - val_loss: 0.0800 - val_accuracy: 0.9785 - lr: 9.0484e-04\n",
            "Epoch 12/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.0830 - val_accuracy: 0.9793 - lr: 8.1873e-04\n",
            "Epoch 13/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 0.0133 - accuracy: 0.9956 - val_loss: 0.0734 - val_accuracy: 0.9808 - lr: 7.4082e-04\n",
            "Epoch 14/100\n",
            "188/188 [==============================] - 19s 104ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.0815 - val_accuracy: 0.9801 - lr: 6.7032e-04\n",
            "Epoch 15/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 0.0743 - val_accuracy: 0.9817 - lr: 6.0653e-04\n",
            "Epoch 16/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0723 - val_accuracy: 0.9836 - lr: 5.4881e-04\n",
            "Epoch 17/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0732 - val_accuracy: 0.9843 - lr: 4.9659e-04\n",
            "Epoch 18/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0721 - val_accuracy: 0.9844 - lr: 4.4933e-04\n",
            "Epoch 19/100\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0805 - val_accuracy: 0.9826 - lr: 4.0657e-04\n",
            "Epoch 20/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 0.0026 - accuracy: 0.9990 - val_loss: 0.0873 - val_accuracy: 0.9816 - lr: 3.6788e-04\n",
            "Epoch 21/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0737 - val_accuracy: 0.9844 - lr: 3.3287e-04\n",
            "Epoch 22/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0761 - val_accuracy: 0.9849 - lr: 3.0119e-04\n",
            "Epoch 23/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 9.0541e-04 - accuracy: 0.9999 - val_loss: 0.0743 - val_accuracy: 0.9860 - lr: 2.7253e-04\n",
            "Epoch 24/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 4.1671e-04 - accuracy: 0.9999 - val_loss: 0.0720 - val_accuracy: 0.9859 - lr: 2.4660e-04\n",
            "Epoch 25/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 1.6745e-04 - accuracy: 1.0000 - val_loss: 0.0733 - val_accuracy: 0.9862 - lr: 2.2313e-04\n",
            "Epoch 26/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 1.4939e-04 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9862 - lr: 2.0190e-04\n",
            "Epoch 27/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 1.8534e-04 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9857 - lr: 1.8268e-04\n",
            "Epoch 28/100\n",
            "188/188 [==============================] - 19s 98ms/step - loss: 1.2361e-04 - accuracy: 1.0000 - val_loss: 0.0731 - val_accuracy: 0.9862 - lr: 1.6530e-04\n",
            "Epoch 29/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 7.1832e-05 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9863 - lr: 1.4957e-04\n",
            "Epoch 30/100\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 6.6745e-05 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9866 - lr: 1.3534e-04\n",
            "Epoch 31/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 6.1479e-05 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9867 - lr: 1.2246e-04\n",
            "Epoch 32/100\n",
            "188/188 [==============================] - 19s 104ms/step - loss: 5.6199e-05 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9862 - lr: 1.1080e-04\n",
            "Epoch 33/100\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 4.5420e-05 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9865 - lr: 1.0026e-04\n",
            "Epoch 34/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 5.2773e-05 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9862 - lr: 9.0718e-05\n",
            "Epoch 35/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 4.4044e-05 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9862 - lr: 8.2085e-05\n",
            "Epoch 36/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 4.4584e-05 - accuracy: 1.0000 - val_loss: 0.0746 - val_accuracy: 0.9865 - lr: 7.4273e-05\n",
            "Epoch 37/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 5.1438e-05 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9868 - lr: 6.7205e-05\n",
            "Epoch 38/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 4.2096e-05 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9869 - lr: 6.0810e-05\n",
            "Epoch 39/100\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 3.7765e-05 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 0.9869 - lr: 5.5023e-05\n",
            "Epoch 40/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 3.5569e-05 - accuracy: 1.0000 - val_loss: 0.0751 - val_accuracy: 0.9871 - lr: 4.9787e-05\n",
            "Epoch 41/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 3.4830e-05 - accuracy: 1.0000 - val_loss: 0.0753 - val_accuracy: 0.9869 - lr: 4.5049e-05\n",
            "Epoch 42/100\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 4.0713e-05 - accuracy: 1.0000 - val_loss: 0.0757 - val_accuracy: 0.9868 - lr: 4.0762e-05\n",
            "Epoch 43/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 4.5064e-05 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9867 - lr: 3.6883e-05\n",
            "Epoch 44/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 3.3404e-05 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9868 - lr: 3.3373e-05\n",
            "Epoch 45/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 3.0255e-05 - accuracy: 1.0000 - val_loss: 0.0763 - val_accuracy: 0.9868 - lr: 3.0197e-05\n",
            "Epoch 46/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 3.5536e-05 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9868 - lr: 2.7324e-05\n",
            "Epoch 47/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 2.8207e-05 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9868 - lr: 2.4723e-05\n",
            "Epoch 48/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 3.0867e-05 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9868 - lr: 2.2371e-05\n",
            "Epoch 49/100\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 2.6695e-05 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9868 - lr: 2.0242e-05\n",
            "Epoch 50/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 2.8073e-05 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9868 - lr: 1.8316e-05\n",
            "Epoch 51/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 2.4032e-05 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9868 - lr: 1.6573e-05\n",
            "Epoch 52/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 2.2701e-05 - accuracy: 1.0000 - val_loss: 0.0768 - val_accuracy: 0.9869 - lr: 1.4996e-05\n",
            "Epoch 53/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 2.5116e-05 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9869 - lr: 1.3569e-05\n",
            "Epoch 54/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 2.3674e-05 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9868 - lr: 1.2277e-05\n",
            "Epoch 55/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 2.3124e-05 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9868 - lr: 1.1109e-05\n",
            "Epoch 56/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 2.0580e-05 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9869 - lr: 1.0052e-05\n",
            "Epoch 57/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 2.1815e-05 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9870 - lr: 9.0953e-06\n",
            "Epoch 58/100\n",
            "188/188 [==============================] - 19s 104ms/step - loss: 2.1436e-05 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9871 - lr: 8.2297e-06\n",
            "Epoch 59/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 2.0246e-05 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9870 - lr: 7.4466e-06\n",
            "Epoch 60/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 1.8738e-05 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9870 - lr: 6.7379e-06\n",
            "Epoch 61/100\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 1.8819e-05 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9870 - lr: 6.0967e-06\n",
            "Epoch 62/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 2.2059e-05 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9869 - lr: 5.5165e-06\n",
            "Epoch 63/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 1.9088e-05 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9869 - lr: 4.9916e-06\n",
            "Epoch 64/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 1.8634e-05 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9868 - lr: 4.5166e-06\n",
            "Epoch 65/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 2.0718e-05 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9869 - lr: 4.0868e-06\n",
            "Epoch 66/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 6.4667e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9868 - lr: 3.6979e-06\n",
            "Epoch 67/100\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 1.9279e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9868 - lr: 3.3460e-06\n",
            "Epoch 68/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 1.8371e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9868 - lr: 3.0275e-06\n",
            "Epoch 69/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 1.9763e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9868 - lr: 2.7394e-06\n",
            "Epoch 70/100\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 2.2599e-05 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9868 - lr: 2.4787e-06\n",
            "Epoch 71/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 1.9608e-05 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9868 - lr: 2.2429e-06\n",
            "Epoch 72/100\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 1.7020e-05 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9868 - lr: 2.0294e-06\n",
            "Epoch 73/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 1.7079e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9868 - lr: 1.8363e-06\n",
            "Epoch 74/100\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 1.7774e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9868 - lr: 1.6616e-06\n",
            "Epoch 75/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 2.0804e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9869 - lr: 1.5034e-06\n",
            "Epoch 76/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 1.6499e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9869 - lr: 1.3604e-06\n",
            "Epoch 77/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 1.7918e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9870 - lr: 1.2309e-06\n",
            "Epoch 78/100\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 1.7079e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9870 - lr: 1.1138e-06\n",
            "Epoch 79/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 1.6683e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9870 - lr: 1.0078e-06\n",
            "Epoch 80/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 1.6773e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9870 - lr: 9.1188e-07\n",
            "Epoch 81/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 1.5538e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9870 - lr: 8.2510e-07\n",
            "Epoch 82/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 1.5890e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9870 - lr: 7.4658e-07\n",
            "Epoch 83/100\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 1.5648e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9870 - lr: 6.7554e-07\n",
            "Epoch 84/100\n",
            "188/188 [==============================] - 19s 100ms/step - loss: 1.5943e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9870 - lr: 6.1125e-07\n",
            "Epoch 85/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 1.6845e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9870 - lr: 5.5308e-07\n",
            "Epoch 86/100\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 1.5233e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9870 - lr: 5.0045e-07\n",
            "Epoch 87/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 1.5139e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9870 - lr: 4.5283e-07\n",
            "Epoch 88/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 1.5300e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9870 - lr: 4.0973e-07\n",
            "Epoch 89/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 1.6468e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9870 - lr: 3.7074e-07\n",
            "Epoch 90/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 1.6764e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 3.3546e-07\n",
            "Epoch 91/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 5.4809e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9870 - lr: 3.0354e-07\n",
            "Epoch 92/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 1.7582e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9870 - lr: 2.7465e-07\n",
            "Epoch 93/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 1.5154e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9870 - lr: 2.4852e-07\n",
            "Epoch 94/100\n",
            "188/188 [==============================] - 19s 101ms/step - loss: 2.0369e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 2.2487e-07\n",
            "Epoch 95/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 1.5758e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 2.0347e-07\n",
            "Epoch 96/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 1.5404e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 1.8410e-07\n",
            "Epoch 97/100\n",
            "188/188 [==============================] - 19s 99ms/step - loss: 1.8630e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 1.6659e-07\n",
            "Epoch 98/100\n",
            "188/188 [==============================] - 19s 104ms/step - loss: 1.5472e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 1.5073e-07\n",
            "Epoch 99/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 1.4926e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 1.3639e-07\n",
            "Epoch 100/100\n",
            "188/188 [==============================] - 18s 98ms/step - loss: 1.6580e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9870 - lr: 1.2341e-07\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cf460379120>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Define your learning rate scheduler function\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr  # Keep the initial learning rate for the first 10 epochs\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)  # Exponentially decay the learning rate after 10 epochs\n",
        "\n",
        "# Create the learning rate scheduler callback\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# Define the model checkpoint callback to save the best model weights\n",
        "checkpoint_filepath = 'model_checkpoint.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Call trans.fit() with the learning rate scheduler and model checkpoint callbacks\n",
        "trans.fit([x_train_ravel, pos_feed_train], y_train,\n",
        "          epochs=100, batch_size=256, validation_split=0.20,\n",
        "          callbacks=[lr_callback, model_checkpoint_callback])"
      ],
      "id": "CJKa2tan1Dw4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeJy66Dq-tY1",
        "outputId": "a3300e46-8e5e-4e13-d30d-3986f824f78e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "#This code will be used for the best transformer (i.e. highest accuracy): saving the model for reproducibility\n",
        "\n",
        "#trans.save(\"/content/drive/MyDrive/optimization_hw_1_t.h5\")"
      ],
      "id": "qeJy66Dq-tY1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66b41b3a",
        "outputId": "76eae494-2edd-424d-8841-88bb98287f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 10s 32ms/step - loss: 0.0705 - accuracy: 0.9880\n"
          ]
        }
      ],
      "source": [
        "out = trans.evaluate([x_test_ravel,pos_feed_test],y_test)"
      ],
      "id": "66b41b3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7de763",
        "outputId": "ba92ba2b-2e6d-422b-e432-a36ae7d1b120"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9879999756813049"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "out[1]"
      ],
      "id": "1a7de763"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0TmCvwYvWAM",
        "outputId": "0aeba9d6-f51f-4806-e14f-92cab6afad33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 16, 49)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "x_train_ravel.shape"
      ],
      "id": "I0TmCvwYvWAM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT Model 2:"
      ],
      "metadata": {
        "id": "Cpm_K48OBQJo"
      },
      "id": "Cpm_K48OBQJo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing the Architecture from ViT 1 to ViT 2:\n",
        "- Increase hidden dimensions slightly (128 to 144) - may increase overfitting byt the model will have a better capacity to learn features\n",
        "- Reduce the number of layers from 16 t 8 (try and offset overfitting this way since the MNIST is not super complex)\n",
        "- Increase the dropout rate from 0.01 to 0.08"
      ],
      "metadata": {
        "id": "pkwfbM-_D0QY"
      },
      "id": "pkwfbM-_D0QY"
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls"
      ],
      "metadata": {
        "id": "kVu-1iFsBPLp"
      },
      "id": "kVu-1iFsBPLp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ViT2(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes):\n",
        "    inp = tf.keras.layers.Input(shape=(n*m,block_size))\n",
        "    inp2 = tf.keras.layers.Input(shape=(n*m))\n",
        "    mid = tf.keras.layers.Dense(hidden_dim)(inp)\n",
        "    emb = tf.keras.layers.Embedding(input_dim=n*m, output_dim=hidden_dim)(inp2)\n",
        "    mid = mid + emb\n",
        "    token = ClassToken()(mid)\n",
        "    mid = tf.keras.layers.Concatenate(axis=1)([token, mid])\n",
        "\n",
        "    for l in range(num_layers):\n",
        "        ln  = tf.keras.layers.LayerNormalization()(mid)\n",
        "        mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,value_dim=key_dim)(ln,ln,ln)\n",
        "        add = tf.keras.layers.Add()([mid,mha])\n",
        "        ln  = tf.keras.layers.LayerNormalization()(add)\n",
        "        den = tf.keras.layers.Dense(mlp_dim,activation='gelu')(ln)\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        den = tf.keras.layers.Dense(hidden_dim)(den)\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        mid = tf.keras.layers.Add()([den,add])\n",
        "    ln = tf.keras.layers.LayerNormalization()(mid)\n",
        "    fl = ln[:,0,:]\n",
        "    clas = tf.keras.layers.Dense(num_classes,activation='softmax')(fl)\n",
        "    mod = tf.keras.models.Model([inp,inp2],clas)\n",
        "    mod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mod"
      ],
      "metadata": {
        "id": "LN-GDmOABPOR"
      },
      "id": "LN-GDmOABPOR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "m = 4\n",
        "block_size = 49\n",
        "hidden_dim = 144\n",
        "num_layers = 8\n",
        "num_heads = 8\n",
        "key_dim = hidden_dim//num_heads\n",
        "mlp_dim = hidden_dim\n",
        "dropout_rate = 0.08\n",
        "num_classes = 10\n",
        "\n",
        "trans = build_ViT2(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
        "trans.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAGMU_poBPQ-",
        "outputId": "8822a559-d195-4303-90dc-d59425fe6a82"
      },
      "id": "zAGMU_poBPQ-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 16, 49)]             0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 16)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 16, 144)              7200      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 16, 144)              2304      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 16, 144)              0         ['dense[0][0]',               \n",
            " Lambda)                                                             'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " class_token (ClassToken)    (None, 1, 144)               144       ['tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 17, 144)              0         ['class_token[0][0]',         \n",
            "                                                                     'tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 17, 144)              288       ['concatenate[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 17, 144)              83520     ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]', \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 17, 144)              0         ['concatenate[0][0]',         \n",
            "                                                                     'multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 17, 144)              288       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 17, 144)              20880     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 17, 144)              0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 17, 144)              20880     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 17, 144)              0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 17, 144)              0         ['dropout_1[0][0]',           \n",
            "                                                                     'add[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 17, 144)              288       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 17, 144)              83520     ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 17, 144)              0         ['add_1[0][0]',               \n",
            "                                                                     'multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 17, 144)              288       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 17, 144)              20880     ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 17, 144)              0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 17, 144)              20880     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 17, 144)              0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 17, 144)              0         ['dropout_3[0][0]',           \n",
            "                                                                     'add_2[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 17, 144)              288       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 17, 144)              83520     ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 17, 144)              0         ['add_3[0][0]',               \n",
            "                                                                     'multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 17, 144)              288       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 17, 144)              20880     ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 17, 144)              0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 17, 144)              20880     ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 17, 144)              0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 17, 144)              0         ['dropout_5[0][0]',           \n",
            "                                                                     'add_4[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 17, 144)              288       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 17, 144)              83520     ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 17, 144)              0         ['add_5[0][0]',               \n",
            "                                                                     'multi_head_attention_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 17, 144)              288       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 17, 144)              20880     ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 17, 144)              0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 17, 144)              20880     ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 17, 144)              0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 17, 144)              0         ['dropout_7[0][0]',           \n",
            "                                                                     'add_6[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 17, 144)              288       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 17, 144)              83520     ['layer_normalization_8[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 17, 144)              0         ['add_7[0][0]',               \n",
            "                                                                     'multi_head_attention_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 17, 144)              288       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 17, 144)              20880     ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 17, 144)              0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 17, 144)              20880     ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 17, 144)              0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 17, 144)              0         ['dropout_9[0][0]',           \n",
            "                                                                     'add_8[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_10 (La  (None, 17, 144)              288       ['add_9[0][0]']               \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 17, 144)              83520     ['layer_normalization_10[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 17, 144)              0         ['add_9[0][0]',               \n",
            "                                                                     'multi_head_attention_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_11 (La  (None, 17, 144)              288       ['add_10[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 17, 144)              20880     ['layer_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 17, 144)              0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 17, 144)              20880     ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 17, 144)              0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 17, 144)              0         ['dropout_11[0][0]',          \n",
            "                                                                     'add_10[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_12 (La  (None, 17, 144)              288       ['add_11[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (Mu  (None, 17, 144)              83520     ['layer_normalization_12[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 17, 144)              0         ['add_11[0][0]',              \n",
            "                                                                     'multi_head_attention_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_13 (La  (None, 17, 144)              288       ['add_12[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 17, 144)              20880     ['layer_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 17, 144)              0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 17, 144)              20880     ['dropout_12[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 17, 144)              0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 17, 144)              0         ['dropout_13[0][0]',          \n",
            "                                                                     'add_12[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_14 (La  (None, 17, 144)              288       ['add_13[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (Mu  (None, 17, 144)              83520     ['layer_normalization_14[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 17, 144)              0         ['add_13[0][0]',              \n",
            "                                                                     'multi_head_attention_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_15 (La  (None, 17, 144)              288       ['add_14[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 17, 144)              20880     ['layer_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 17, 144)              0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 17, 144)              20880     ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)        (None, 17, 144)              0         ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 17, 144)              0         ['dropout_15[0][0]',          \n",
            "                                                                     'add_14[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_16 (La  (None, 17, 144)              288       ['add_15[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 144)                  0         ['layer_normalization_16[0][0]\n",
            " SlicingOpLambda)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 10)                   1450      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1018234 (3.88 MB)\n",
            "Trainable params: 1018234 (3.88 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "ndata_train = x_train.shape[0]\n",
        "ndata_test = x_test.shape[0]"
      ],
      "metadata": {
        "id": "vL6g8sxKBPTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148934a4-2a42-4be1-c064-4c60970e47e4"
      },
      "id": "vL6g8sxKBPTg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ravel = np.zeros((ndata_train,n*m,block_size))\n",
        "for img in range(ndata_train):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_train_ravel[img,ind,:] = x_train[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1"
      ],
      "metadata": {
        "id": "hlJL4kpjBPVW"
      },
      "id": "hlJL4kpjBPVW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_ravel = np.zeros((ndata_test,n*m,block_size))\n",
        "for img in range(ndata_test):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_test_ravel[img,ind,:] = x_test[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1"
      ],
      "metadata": {
        "id": "jSOiU5jpBPYP"
      },
      "id": "jSOiU5jpBPYP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
        "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
      ],
      "metadata": {
        "id": "lMPa-FZPBPax"
      },
      "id": "lMPa-FZPBPax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The training procedure and additional callbacks will be KEPT THE SAME for Model 1 to Model 2:\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "checkpoint_filepath = 'model_checkpoint.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "trans.fit([x_train_ravel, pos_feed_train], y_train,\n",
        "          epochs=100, batch_size=256, validation_split=0.20,\n",
        "          callbacks=[lr_callback, model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNhGnoUbBPdv",
        "outputId": "be10e73c-55c9-4fc9-8d97-fba922973a43"
      },
      "id": "bNhGnoUbBPdv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "188/188 [==============================] - 13s 70ms/step - loss: 0.1961 - accuracy: 0.9390 - val_loss: 0.1295 - val_accuracy: 0.9628 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 15s 80ms/step - loss: 0.1232 - accuracy: 0.9611 - val_loss: 0.1061 - val_accuracy: 0.9698 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0979 - accuracy: 0.9687 - val_loss: 0.0981 - val_accuracy: 0.9714 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0786 - accuracy: 0.9754 - val_loss: 0.0967 - val_accuracy: 0.9725 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 13s 71ms/step - loss: 0.0680 - accuracy: 0.9787 - val_loss: 0.0868 - val_accuracy: 0.9750 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0624 - accuracy: 0.9799 - val_loss: 0.0862 - val_accuracy: 0.9757 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "188/188 [==============================] - 12s 62ms/step - loss: 0.0556 - accuracy: 0.9819 - val_loss: 0.0917 - val_accuracy: 0.9747 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0511 - accuracy: 0.9829 - val_loss: 0.0861 - val_accuracy: 0.9761 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0515 - accuracy: 0.9831 - val_loss: 0.0842 - val_accuracy: 0.9743 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0478 - accuracy: 0.9845 - val_loss: 0.0861 - val_accuracy: 0.9753 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0391 - accuracy: 0.9868 - val_loss: 0.0842 - val_accuracy: 0.9783 - lr: 9.0484e-04\n",
            "Epoch 12/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0296 - accuracy: 0.9902 - val_loss: 0.0669 - val_accuracy: 0.9823 - lr: 8.1873e-04\n",
            "Epoch 13/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.0678 - val_accuracy: 0.9816 - lr: 7.4082e-04\n",
            "Epoch 14/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 0.0222 - accuracy: 0.9919 - val_loss: 0.0659 - val_accuracy: 0.9820 - lr: 6.7032e-04\n",
            "Epoch 15/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.0657 - val_accuracy: 0.9836 - lr: 6.0653e-04\n",
            "Epoch 16/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.0670 - val_accuracy: 0.9833 - lr: 5.4881e-04\n",
            "Epoch 17/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0662 - val_accuracy: 0.9836 - lr: 4.9659e-04\n",
            "Epoch 18/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0089 - accuracy: 0.9967 - val_loss: 0.0714 - val_accuracy: 0.9831 - lr: 4.4933e-04\n",
            "Epoch 19/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0652 - val_accuracy: 0.9847 - lr: 4.0657e-04\n",
            "Epoch 20/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0664 - val_accuracy: 0.9854 - lr: 3.6788e-04\n",
            "Epoch 21/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0666 - val_accuracy: 0.9858 - lr: 3.3287e-04\n",
            "Epoch 22/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0688 - val_accuracy: 0.9847 - lr: 3.0119e-04\n",
            "Epoch 23/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0707 - val_accuracy: 0.9847 - lr: 2.7253e-04\n",
            "Epoch 24/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0729 - val_accuracy: 0.9839 - lr: 2.4660e-04\n",
            "Epoch 25/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0730 - val_accuracy: 0.9855 - lr: 2.2313e-04\n",
            "Epoch 26/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0690 - val_accuracy: 0.9854 - lr: 2.0190e-04\n",
            "Epoch 27/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0692 - val_accuracy: 0.9872 - lr: 1.8268e-04\n",
            "Epoch 28/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0718 - val_accuracy: 0.9868 - lr: 1.6530e-04\n",
            "Epoch 29/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0709 - val_accuracy: 0.9861 - lr: 1.4957e-04\n",
            "Epoch 30/100\n",
            "188/188 [==============================] - 13s 71ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0728 - val_accuracy: 0.9857 - lr: 1.3534e-04\n",
            "Epoch 31/100\n",
            "188/188 [==============================] - 13s 71ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0691 - val_accuracy: 0.9860 - lr: 1.2246e-04\n",
            "Epoch 32/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 8.3019e-04 - accuracy: 0.9998 - val_loss: 0.0702 - val_accuracy: 0.9865 - lr: 1.1080e-04\n",
            "Epoch 33/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 6.8217e-04 - accuracy: 0.9999 - val_loss: 0.0687 - val_accuracy: 0.9872 - lr: 1.0026e-04\n",
            "Epoch 34/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 6.3533e-04 - accuracy: 0.9999 - val_loss: 0.0695 - val_accuracy: 0.9870 - lr: 9.0718e-05\n",
            "Epoch 35/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 7.1636e-04 - accuracy: 0.9998 - val_loss: 0.0701 - val_accuracy: 0.9865 - lr: 8.2085e-05\n",
            "Epoch 36/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 6.3974e-04 - accuracy: 0.9998 - val_loss: 0.0696 - val_accuracy: 0.9872 - lr: 7.4273e-05\n",
            "Epoch 37/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 5.2198e-04 - accuracy: 0.9999 - val_loss: 0.0699 - val_accuracy: 0.9871 - lr: 6.7205e-05\n",
            "Epoch 38/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 6.0859e-04 - accuracy: 0.9999 - val_loss: 0.0688 - val_accuracy: 0.9871 - lr: 6.0810e-05\n",
            "Epoch 39/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 7.8461e-04 - accuracy: 0.9997 - val_loss: 0.0714 - val_accuracy: 0.9868 - lr: 5.5023e-05\n",
            "Epoch 40/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 5.1422e-04 - accuracy: 0.9999 - val_loss: 0.0710 - val_accuracy: 0.9871 - lr: 4.9787e-05\n",
            "Epoch 41/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 3.8681e-04 - accuracy: 0.9999 - val_loss: 0.0697 - val_accuracy: 0.9869 - lr: 4.5049e-05\n",
            "Epoch 42/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 3.0296e-04 - accuracy: 0.9999 - val_loss: 0.0712 - val_accuracy: 0.9866 - lr: 4.0762e-05\n",
            "Epoch 43/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 4.0275e-04 - accuracy: 0.9999 - val_loss: 0.0709 - val_accuracy: 0.9862 - lr: 3.6883e-05\n",
            "Epoch 44/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 2.4707e-04 - accuracy: 1.0000 - val_loss: 0.0719 - val_accuracy: 0.9868 - lr: 3.3373e-05\n",
            "Epoch 45/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 2.4985e-04 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9872 - lr: 3.0197e-05\n",
            "Epoch 46/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 2.6859e-04 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9868 - lr: 2.7324e-05\n",
            "Epoch 47/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 2.0654e-04 - accuracy: 1.0000 - val_loss: 0.0709 - val_accuracy: 0.9872 - lr: 2.4723e-05\n",
            "Epoch 48/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.7643e-04 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9872 - lr: 2.2371e-05\n",
            "Epoch 49/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.8974e-04 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9870 - lr: 2.0242e-05\n",
            "Epoch 50/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 2.0969e-04 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 0.9868 - lr: 1.8316e-05\n",
            "Epoch 51/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 2.2480e-04 - accuracy: 0.9999 - val_loss: 0.0714 - val_accuracy: 0.9872 - lr: 1.6573e-05\n",
            "Epoch 52/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 2.9535e-04 - accuracy: 0.9999 - val_loss: 0.0730 - val_accuracy: 0.9869 - lr: 1.4996e-05\n",
            "Epoch 53/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 2.4006e-04 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9869 - lr: 1.3569e-05\n",
            "Epoch 54/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.7758e-04 - accuracy: 1.0000 - val_loss: 0.0719 - val_accuracy: 0.9870 - lr: 1.2277e-05\n",
            "Epoch 55/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.6882e-04 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 0.9872 - lr: 1.1109e-05\n",
            "Epoch 56/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.9046e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 1.0052e-05\n",
            "Epoch 57/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 2.0136e-04 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9871 - lr: 9.0953e-06\n",
            "Epoch 58/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 2.0085e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9872 - lr: 8.2297e-06\n",
            "Epoch 59/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.7118e-04 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9872 - lr: 7.4466e-06\n",
            "Epoch 60/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 2.1069e-04 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9873 - lr: 6.7379e-06\n",
            "Epoch 61/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.3755e-04 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9874 - lr: 6.0967e-06\n",
            "Epoch 62/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.7777e-04 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9875 - lr: 5.5165e-06\n",
            "Epoch 63/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 2.5036e-04 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9873 - lr: 4.9916e-06\n",
            "Epoch 64/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 9.9347e-05 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9874 - lr: 4.5166e-06\n",
            "Epoch 65/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 2.1334e-04 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9872 - lr: 4.0868e-06\n",
            "Epoch 66/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3652e-04 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9873 - lr: 3.6979e-06\n",
            "Epoch 67/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 2.0521e-04 - accuracy: 0.9999 - val_loss: 0.0726 - val_accuracy: 0.9872 - lr: 3.3460e-06\n",
            "Epoch 68/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.5873e-04 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9872 - lr: 3.0275e-06\n",
            "Epoch 69/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.7950e-04 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9873 - lr: 2.7394e-06\n",
            "Epoch 70/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.5150e-04 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9872 - lr: 2.4787e-06\n",
            "Epoch 71/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.7929e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9872 - lr: 2.2429e-06\n",
            "Epoch 72/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.5386e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9872 - lr: 2.0294e-06\n",
            "Epoch 73/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.2694e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9872 - lr: 1.8363e-06\n",
            "Epoch 74/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.4500e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 1.6616e-06\n",
            "Epoch 75/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 1.2183e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9874 - lr: 1.5034e-06\n",
            "Epoch 76/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3804e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9874 - lr: 1.3604e-06\n",
            "Epoch 77/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 1.5877e-04 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 0.9872 - lr: 1.2309e-06\n",
            "Epoch 78/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3590e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 1.1138e-06\n",
            "Epoch 79/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.0543e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 1.0078e-06\n",
            "Epoch 80/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.6708e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 9.1188e-07\n",
            "Epoch 81/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 1.9293e-04 - accuracy: 0.9999 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 8.2510e-07\n",
            "Epoch 82/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 1.2952e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 7.4658e-07\n",
            "Epoch 83/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3393e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 6.7554e-07\n",
            "Epoch 84/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3732e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 6.1125e-07\n",
            "Epoch 85/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3301e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 5.5308e-07\n",
            "Epoch 86/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3735e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 5.0045e-07\n",
            "Epoch 87/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.3893e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 4.5283e-07\n",
            "Epoch 88/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.1519e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 4.0973e-07\n",
            "Epoch 89/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.2605e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 3.7074e-07\n",
            "Epoch 90/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.8204e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 3.3546e-07\n",
            "Epoch 91/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 1.3095e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 3.0354e-07\n",
            "Epoch 92/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.4689e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9872 - lr: 2.7465e-07\n",
            "Epoch 93/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.2658e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9873 - lr: 2.4852e-07\n",
            "Epoch 94/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 1.7168e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 2.2487e-07\n",
            "Epoch 95/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 2.0808e-04 - accuracy: 0.9999 - val_loss: 0.0724 - val_accuracy: 0.9872 - lr: 2.0347e-07\n",
            "Epoch 96/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.5973e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9872 - lr: 1.8410e-07\n",
            "Epoch 97/100\n",
            "188/188 [==============================] - 12s 63ms/step - loss: 1.4458e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9872 - lr: 1.6659e-07\n",
            "Epoch 98/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.1860e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9872 - lr: 1.5073e-07\n",
            "Epoch 99/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.8457e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9872 - lr: 1.3639e-07\n",
            "Epoch 100/100\n",
            "188/188 [==============================] - 12s 64ms/step - loss: 1.2149e-04 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9872 - lr: 1.2341e-07\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d59385d3d00>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = trans.evaluate([x_test_ravel,pos_feed_test],y_test)"
      ],
      "metadata": {
        "id": "bMdxt5miBPi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3dbfcd6-d7dd-42e2-ef30-9e81dc4ecf04"
      },
      "id": "bMdxt5miBPi4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0684 - accuracy: 0.9866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[1]"
      ],
      "metadata": {
        "id": "p0b33ahZBPkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198e0605-9a8f-4ee0-c490-25ac075abcdc"
      },
      "id": "p0b33ahZBPkZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9865999817848206"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ravel.shape"
      ],
      "metadata": {
        "id": "Lw5dsPSmCTOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20c5849-cd67-47dc-bb0e-4a2e0c249179"
      },
      "id": "Lw5dsPSmCTOH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 16, 49)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT Model 3:"
      ],
      "metadata": {
        "id": "VYeoQQX-Ea_f"
      },
      "id": "VYeoQQX-Ea_f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to revert back to a dropout rate of 0.01 here and see what changes. Aside from this minor change everything is the same as Model 2 (in other words we have kept the increased hidden dimensions and reduced layers)."
      ],
      "metadata": {
        "id": "eTRJWE86E_3s"
      },
      "id": "eTRJWE86E_3s"
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls"
      ],
      "metadata": {
        "id": "Fu4aaJFOCTWv"
      },
      "id": "Fu4aaJFOCTWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ViT3(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes):\n",
        "    inp = tf.keras.layers.Input(shape=(n*m,block_size))\n",
        "    inp2 = tf.keras.layers.Input(shape=(n*m))\n",
        "    mid = tf.keras.layers.Dense(hidden_dim)(inp)\n",
        "    emb = tf.keras.layers.Embedding(input_dim=n*m, output_dim=hidden_dim)(inp2)\n",
        "    mid = mid + emb\n",
        "    token = ClassToken()(mid)\n",
        "    mid = tf.keras.layers.Concatenate(axis=1)([token, mid])\n",
        "\n",
        "    for l in range(num_layers):\n",
        "        ln  = tf.keras.layers.LayerNormalization()(mid)\n",
        "        mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,value_dim=key_dim)(ln,ln,ln)\n",
        "        add = tf.keras.layers.Add()([mid,mha])\n",
        "        ln  = tf.keras.layers.LayerNormalization()(add)\n",
        "        den = tf.keras.layers.Dense(mlp_dim,activation='gelu')(ln)\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        den = tf.keras.layers.Dense(hidden_dim)(den)\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        mid = tf.keras.layers.Add()([den,add])\n",
        "    ln = tf.keras.layers.LayerNormalization()(mid)\n",
        "    fl = ln[:,0,:]\n",
        "    clas = tf.keras.layers.Dense(num_classes,activation='softmax')(fl)\n",
        "    mod = tf.keras.models.Model([inp,inp2],clas)\n",
        "    mod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mod"
      ],
      "metadata": {
        "id": "oT-2MXsCCTZN"
      },
      "id": "oT-2MXsCCTZN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "m = 4\n",
        "block_size = 49\n",
        "hidden_dim = 144\n",
        "num_layers = 8\n",
        "num_heads = 8\n",
        "key_dim = hidden_dim//num_heads\n",
        "mlp_dim = hidden_dim\n",
        "dropout_rate = 0.01\n",
        "num_classes = 10\n",
        "\n",
        "trans = build_ViT3(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
        "trans.summary()"
      ],
      "metadata": {
        "id": "wQO7SLlgCTcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c7cadd-b8ae-423b-b842-9235d576c74d"
      },
      "id": "wQO7SLlgCTcA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 16, 49)]             0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 16)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 16, 144)              7200      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 16, 144)              2304      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 16, 144)              0         ['dense[0][0]',               \n",
            " Lambda)                                                             'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " class_token (ClassToken)    (None, 1, 144)               144       ['tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 17, 144)              0         ['class_token[0][0]',         \n",
            "                                                                     'tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 17, 144)              288       ['concatenate[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 17, 144)              83520     ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]', \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 17, 144)              0         ['concatenate[0][0]',         \n",
            "                                                                     'multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 17, 144)              288       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 17, 144)              20880     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 17, 144)              0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 17, 144)              20880     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 17, 144)              0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 17, 144)              0         ['dropout_1[0][0]',           \n",
            "                                                                     'add[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 17, 144)              288       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 17, 144)              83520     ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 17, 144)              0         ['add_1[0][0]',               \n",
            "                                                                     'multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 17, 144)              288       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 17, 144)              20880     ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 17, 144)              0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 17, 144)              20880     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 17, 144)              0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 17, 144)              0         ['dropout_3[0][0]',           \n",
            "                                                                     'add_2[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 17, 144)              288       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 17, 144)              83520     ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 17, 144)              0         ['add_3[0][0]',               \n",
            "                                                                     'multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 17, 144)              288       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 17, 144)              20880     ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 17, 144)              0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 17, 144)              20880     ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 17, 144)              0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 17, 144)              0         ['dropout_5[0][0]',           \n",
            "                                                                     'add_4[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 17, 144)              288       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 17, 144)              83520     ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 17, 144)              0         ['add_5[0][0]',               \n",
            "                                                                     'multi_head_attention_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 17, 144)              288       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 17, 144)              20880     ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 17, 144)              0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 17, 144)              20880     ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 17, 144)              0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 17, 144)              0         ['dropout_7[0][0]',           \n",
            "                                                                     'add_6[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 17, 144)              288       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 17, 144)              83520     ['layer_normalization_8[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 17, 144)              0         ['add_7[0][0]',               \n",
            "                                                                     'multi_head_attention_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 17, 144)              288       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 17, 144)              20880     ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 17, 144)              0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 17, 144)              20880     ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 17, 144)              0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 17, 144)              0         ['dropout_9[0][0]',           \n",
            "                                                                     'add_8[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_10 (La  (None, 17, 144)              288       ['add_9[0][0]']               \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 17, 144)              83520     ['layer_normalization_10[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 17, 144)              0         ['add_9[0][0]',               \n",
            "                                                                     'multi_head_attention_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_11 (La  (None, 17, 144)              288       ['add_10[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 17, 144)              20880     ['layer_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 17, 144)              0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 17, 144)              20880     ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 17, 144)              0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 17, 144)              0         ['dropout_11[0][0]',          \n",
            "                                                                     'add_10[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_12 (La  (None, 17, 144)              288       ['add_11[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (Mu  (None, 17, 144)              83520     ['layer_normalization_12[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 17, 144)              0         ['add_11[0][0]',              \n",
            "                                                                     'multi_head_attention_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_13 (La  (None, 17, 144)              288       ['add_12[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 17, 144)              20880     ['layer_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 17, 144)              0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 17, 144)              20880     ['dropout_12[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 17, 144)              0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 17, 144)              0         ['dropout_13[0][0]',          \n",
            "                                                                     'add_12[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_14 (La  (None, 17, 144)              288       ['add_13[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (Mu  (None, 17, 144)              83520     ['layer_normalization_14[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 17, 144)              0         ['add_13[0][0]',              \n",
            "                                                                     'multi_head_attention_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_15 (La  (None, 17, 144)              288       ['add_14[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 17, 144)              20880     ['layer_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 17, 144)              0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 17, 144)              20880     ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)        (None, 17, 144)              0         ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 17, 144)              0         ['dropout_15[0][0]',          \n",
            "                                                                     'add_14[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_16 (La  (None, 17, 144)              288       ['add_15[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 144)                  0         ['layer_normalization_16[0][0]\n",
            " SlicingOpLambda)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 10)                   1450      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1018234 (3.88 MB)\n",
            "Trainable params: 1018234 (3.88 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "ndata_train = x_train.shape[0]\n",
        "ndata_test = x_test.shape[0]"
      ],
      "metadata": {
        "id": "mEvOuEyEEffW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112ee78b-26ce-4508-fe47-b61a1e92469b"
      },
      "id": "mEvOuEyEEffW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ravel = np.zeros((ndata_train,n*m,block_size))\n",
        "for img in range(ndata_train):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_train_ravel[img,ind,:] = x_train[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1"
      ],
      "metadata": {
        "id": "NpoPeXjTEfiG"
      },
      "id": "NpoPeXjTEfiG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_ravel = np.zeros((ndata_test,n*m,block_size))\n",
        "for img in range(ndata_test):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_test_ravel[img,ind,:] = x_test[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1"
      ],
      "metadata": {
        "id": "QGYxmefIEfkZ"
      },
      "id": "QGYxmefIEfkZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
        "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
      ],
      "metadata": {
        "id": "Iw3mE7wmEfn1"
      },
      "id": "Iw3mE7wmEfn1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The training procedure and additional callbacks will be KEPT THE SAME AGAIN\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "checkpoint_filepath = 'model_checkpoint.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "trans.fit([x_train_ravel, pos_feed_train], y_train,\n",
        "          epochs=100, batch_size=256, validation_split=0.20,\n",
        "          callbacks=[lr_callback, model_checkpoint_callback])"
      ],
      "metadata": {
        "id": "ac0XK9-3Efpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e7b8cb-b73f-48d8-c7bd-8ad52ab5cd15"
      },
      "id": "ac0XK9-3Efpc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "188/188 [==============================] - 40s 74ms/step - loss: 0.9608 - accuracy: 0.6755 - val_loss: 0.2357 - val_accuracy: 0.9293 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1637 - accuracy: 0.9496 - val_loss: 0.1358 - val_accuracy: 0.9601 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 0.1036 - accuracy: 0.9677 - val_loss: 0.1257 - val_accuracy: 0.9634 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 0.0785 - accuracy: 0.9746 - val_loss: 0.1147 - val_accuracy: 0.9674 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0618 - accuracy: 0.9801 - val_loss: 0.1058 - val_accuracy: 0.9704 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0574 - accuracy: 0.9808 - val_loss: 0.0976 - val_accuracy: 0.9721 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0467 - accuracy: 0.9849 - val_loss: 0.1095 - val_accuracy: 0.9681 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0427 - accuracy: 0.9856 - val_loss: 0.0834 - val_accuracy: 0.9763 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0368 - accuracy: 0.9876 - val_loss: 0.0858 - val_accuracy: 0.9749 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0334 - accuracy: 0.9885 - val_loss: 0.0937 - val_accuracy: 0.9758 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 0.0311 - accuracy: 0.9898 - val_loss: 0.0770 - val_accuracy: 0.9774 - lr: 9.0484e-04\n",
            "Epoch 12/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 0.0232 - accuracy: 0.9918 - val_loss: 0.0772 - val_accuracy: 0.9793 - lr: 8.1873e-04\n",
            "Epoch 13/100\n",
            "188/188 [==============================] - 14s 73ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0970 - val_accuracy: 0.9767 - lr: 7.4082e-04\n",
            "Epoch 14/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0914 - val_accuracy: 0.9785 - lr: 6.7032e-04\n",
            "Epoch 15/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 0.0117 - accuracy: 0.9959 - val_loss: 0.0768 - val_accuracy: 0.9812 - lr: 6.0653e-04\n",
            "Epoch 16/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0839 - val_accuracy: 0.9815 - lr: 5.4881e-04\n",
            "Epoch 17/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0728 - val_accuracy: 0.9837 - lr: 4.9659e-04\n",
            "Epoch 18/100\n",
            "188/188 [==============================] - 14s 74ms/step - loss: 8.1901e-04 - accuracy: 0.9999 - val_loss: 0.0725 - val_accuracy: 0.9841 - lr: 4.4933e-04\n",
            "Epoch 19/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.8719e-04 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9855 - lr: 4.0657e-04\n",
            "Epoch 20/100\n",
            "188/188 [==============================] - 17s 90ms/step - loss: 1.7210e-04 - accuracy: 1.0000 - val_loss: 0.0733 - val_accuracy: 0.9855 - lr: 3.6788e-04\n",
            "Epoch 21/100\n",
            "188/188 [==============================] - 15s 79ms/step - loss: 1.5544e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9857 - lr: 3.3287e-04\n",
            "Epoch 22/100\n",
            "188/188 [==============================] - 15s 80ms/step - loss: 1.3366e-04 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9857 - lr: 3.0119e-04\n",
            "Epoch 23/100\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 1.3804e-04 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9857 - lr: 2.7253e-04\n",
            "Epoch 24/100\n",
            "188/188 [==============================] - 15s 78ms/step - loss: 1.0093e-04 - accuracy: 1.0000 - val_loss: 0.0749 - val_accuracy: 0.9858 - lr: 2.4660e-04\n",
            "Epoch 25/100\n",
            "188/188 [==============================] - 13s 72ms/step - loss: 9.1731e-05 - accuracy: 1.0000 - val_loss: 0.0755 - val_accuracy: 0.9858 - lr: 2.2313e-04\n",
            "Epoch 26/100\n",
            "188/188 [==============================] - 16s 84ms/step - loss: 8.4102e-05 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9855 - lr: 2.0190e-04\n",
            "Epoch 27/100\n",
            "188/188 [==============================] - 15s 82ms/step - loss: 7.7538e-05 - accuracy: 1.0000 - val_loss: 0.0782 - val_accuracy: 0.9855 - lr: 1.8268e-04\n",
            "Epoch 28/100\n",
            "188/188 [==============================] - 13s 71ms/step - loss: 6.8867e-05 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 0.9858 - lr: 1.6530e-04\n",
            "Epoch 29/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 6.4937e-05 - accuracy: 1.0000 - val_loss: 0.0771 - val_accuracy: 0.9858 - lr: 1.4957e-04\n",
            "Epoch 30/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 5.8385e-05 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9860 - lr: 1.3534e-04\n",
            "Epoch 31/100\n",
            "188/188 [==============================] - 17s 93ms/step - loss: 5.6969e-05 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9859 - lr: 1.2246e-04\n",
            "Epoch 32/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 5.3644e-05 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9859 - lr: 1.1080e-04\n",
            "Epoch 33/100\n",
            "188/188 [==============================] - 13s 70ms/step - loss: 7.4755e-05 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9862 - lr: 1.0026e-04\n",
            "Epoch 34/100\n",
            "188/188 [==============================] - 14s 73ms/step - loss: 5.6649e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9858 - lr: 9.0718e-05\n",
            "Epoch 35/100\n",
            "188/188 [==============================] - 17s 90ms/step - loss: 5.8057e-05 - accuracy: 1.0000 - val_loss: 0.0796 - val_accuracy: 0.9858 - lr: 8.2085e-05\n",
            "Epoch 36/100\n",
            "188/188 [==============================] - 14s 72ms/step - loss: 4.6963e-05 - accuracy: 1.0000 - val_loss: 0.0787 - val_accuracy: 0.9860 - lr: 7.4273e-05\n",
            "Epoch 37/100\n",
            "188/188 [==============================] - 14s 77ms/step - loss: 4.3981e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9862 - lr: 6.7205e-05\n",
            "Epoch 38/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 4.4953e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9860 - lr: 6.0810e-05\n",
            "Epoch 39/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 4.1930e-05 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 0.9858 - lr: 5.5023e-05\n",
            "Epoch 40/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 3.6339e-05 - accuracy: 1.0000 - val_loss: 0.0794 - val_accuracy: 0.9858 - lr: 4.9787e-05\n",
            "Epoch 41/100\n",
            "188/188 [==============================] - 14s 73ms/step - loss: 4.6174e-05 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9852 - lr: 4.5049e-05\n",
            "Epoch 42/100\n",
            "188/188 [==============================] - 15s 81ms/step - loss: 3.8066e-05 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 0.9857 - lr: 4.0762e-05\n",
            "Epoch 43/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 6.8141e-05 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9860 - lr: 3.6883e-05\n",
            "Epoch 44/100\n",
            "188/188 [==============================] - 14s 73ms/step - loss: 3.6888e-05 - accuracy: 1.0000 - val_loss: 0.0794 - val_accuracy: 0.9861 - lr: 3.3373e-05\n",
            "Epoch 45/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 3.3876e-05 - accuracy: 1.0000 - val_loss: 0.0795 - val_accuracy: 0.9861 - lr: 3.0197e-05\n",
            "Epoch 46/100\n",
            "188/188 [==============================] - 12s 65ms/step - loss: 3.1860e-05 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9860 - lr: 2.7324e-05\n",
            "Epoch 47/100\n",
            "188/188 [==============================] - 13s 71ms/step - loss: 7.7132e-05 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9854 - lr: 2.4723e-05\n",
            "Epoch 48/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 4.3144e-05 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9857 - lr: 2.2371e-05\n",
            "Epoch 49/100\n",
            "188/188 [==============================] - 14s 72ms/step - loss: 3.7659e-05 - accuracy: 1.0000 - val_loss: 0.0818 - val_accuracy: 0.9858 - lr: 2.0242e-05\n",
            "Epoch 50/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 4.0224e-05 - accuracy: 1.0000 - val_loss: 0.0818 - val_accuracy: 0.9858 - lr: 1.8316e-05\n",
            "Epoch 51/100\n",
            "188/188 [==============================] - 14s 72ms/step - loss: 3.5424e-05 - accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 0.9858 - lr: 1.6573e-05\n",
            "Epoch 52/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 3.3776e-05 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9859 - lr: 1.4996e-05\n",
            "Epoch 53/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 3.0974e-05 - accuracy: 1.0000 - val_loss: 0.0812 - val_accuracy: 0.9861 - lr: 1.3569e-05\n",
            "Epoch 54/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 9.2509e-05 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9858 - lr: 1.2277e-05\n",
            "Epoch 55/100\n",
            "188/188 [==============================] - 16s 84ms/step - loss: 3.4129e-05 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9858 - lr: 1.1109e-05\n",
            "Epoch 56/100\n",
            "188/188 [==============================] - 13s 71ms/step - loss: 4.2771e-05 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9855 - lr: 1.0052e-05\n",
            "Epoch 57/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 2.7314e-05 - accuracy: 1.0000 - val_loss: 0.0828 - val_accuracy: 0.9856 - lr: 9.0953e-06\n",
            "Epoch 58/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.8156e-05 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9858 - lr: 8.2297e-06\n",
            "Epoch 59/100\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 2.8493e-05 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9858 - lr: 7.4466e-06\n",
            "Epoch 60/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 2.8839e-05 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9856 - lr: 6.7379e-06\n",
            "Epoch 61/100\n",
            "188/188 [==============================] - 13s 70ms/step - loss: 2.9042e-05 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9856 - lr: 6.0967e-06\n",
            "Epoch 62/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.5979e-05 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9857 - lr: 5.5165e-06\n",
            "Epoch 63/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.9395e-05 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9858 - lr: 4.9916e-06\n",
            "Epoch 64/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.6466e-05 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9857 - lr: 4.5166e-06\n",
            "Epoch 65/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.5886e-05 - accuracy: 1.0000 - val_loss: 0.0829 - val_accuracy: 0.9858 - lr: 4.0868e-06\n",
            "Epoch 66/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.6413e-05 - accuracy: 1.0000 - val_loss: 0.0829 - val_accuracy: 0.9858 - lr: 3.6979e-06\n",
            "Epoch 67/100\n",
            "188/188 [==============================] - 13s 70ms/step - loss: 2.3525e-05 - accuracy: 1.0000 - val_loss: 0.0829 - val_accuracy: 0.9858 - lr: 3.3460e-06\n",
            "Epoch 68/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.3471e-05 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9859 - lr: 3.0275e-06\n",
            "Epoch 69/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.4790e-05 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9858 - lr: 2.7394e-06\n",
            "Epoch 70/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.3571e-05 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9858 - lr: 2.4787e-06\n",
            "Epoch 71/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 2.6190e-05 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9858 - lr: 2.2429e-06\n",
            "Epoch 72/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.4748e-05 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9860 - lr: 2.0294e-06\n",
            "Epoch 73/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.1686e-05 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 0.9859 - lr: 1.8363e-06\n",
            "Epoch 74/100\n",
            "188/188 [==============================] - 13s 70ms/step - loss: 2.1927e-05 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 0.9859 - lr: 1.6616e-06\n",
            "Epoch 75/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.1460e-05 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9858 - lr: 1.5034e-06\n",
            "Epoch 76/100\n",
            "188/188 [==============================] - 13s 70ms/step - loss: 2.1825e-05 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9858 - lr: 1.3604e-06\n",
            "Epoch 77/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.1691e-05 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9859 - lr: 1.2309e-06\n",
            "Epoch 78/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 2.5640e-05 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9858 - lr: 1.1138e-06\n",
            "Epoch 79/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.1895e-05 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9858 - lr: 1.0078e-06\n",
            "Epoch 80/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.3187e-05 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9857 - lr: 9.1188e-07\n",
            "Epoch 81/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.8963e-05 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9858 - lr: 8.2510e-07\n",
            "Epoch 82/100\n",
            "188/188 [==============================] - 14s 76ms/step - loss: 2.1752e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9858 - lr: 7.4658e-07\n",
            "Epoch 83/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.0273e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9858 - lr: 6.7554e-07\n",
            "Epoch 84/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.1179e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9858 - lr: 6.1125e-07\n",
            "Epoch 85/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 1.9826e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9859 - lr: 5.5308e-07\n",
            "Epoch 86/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 2.1495e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9858 - lr: 5.0045e-07\n",
            "Epoch 87/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.1278e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9858 - lr: 4.5283e-07\n",
            "Epoch 88/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.0899e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9859 - lr: 4.0973e-07\n",
            "Epoch 89/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.0452e-05 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9858 - lr: 3.7074e-07\n",
            "Epoch 90/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.0549e-05 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9858 - lr: 3.3546e-07\n",
            "Epoch 91/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.0425e-05 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9858 - lr: 3.0354e-07\n",
            "Epoch 92/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 2.0394e-05 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9858 - lr: 2.7465e-07\n",
            "Epoch 93/100\n",
            "188/188 [==============================] - 13s 68ms/step - loss: 2.1070e-05 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9859 - lr: 2.4852e-07\n",
            "Epoch 94/100\n",
            "188/188 [==============================] - 14s 75ms/step - loss: 2.0227e-05 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9859 - lr: 2.2487e-07\n",
            "Epoch 95/100\n",
            "188/188 [==============================] - 13s 67ms/step - loss: 2.0004e-05 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9859 - lr: 2.0347e-07\n",
            "Epoch 96/100\n",
            "188/188 [==============================] - 14s 73ms/step - loss: 2.0627e-05 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9859 - lr: 1.8410e-07\n",
            "Epoch 97/100\n",
            "188/188 [==============================] - 14s 73ms/step - loss: 1.9515e-05 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9859 - lr: 1.6659e-07\n",
            "Epoch 98/100\n",
            "188/188 [==============================] - 13s 69ms/step - loss: 1.8393e-05 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9859 - lr: 1.5073e-07\n",
            "Epoch 99/100\n",
            "188/188 [==============================] - 12s 66ms/step - loss: 2.0331e-05 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9859 - lr: 1.3639e-07\n",
            "Epoch 100/100\n",
            "188/188 [==============================] - 14s 72ms/step - loss: 2.0464e-05 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9859 - lr: 1.2341e-07\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e9367ff15d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model for reproducibility\n",
        "\n",
        "trans.save(\"/content/drive/MyDrive/transformer_3.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAKBJGz3urjW",
        "outputId": "89d39db7-d2a0-414d-f745-5b24be90693b"
      },
      "id": "IAKBJGz3urjW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = trans.evaluate([x_test_ravel,pos_feed_test],y_test)"
      ],
      "metadata": {
        "id": "E6VzbBl0Efsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7b07db-8a03-45f6-f8bd-77fcba47dc89"
      },
      "id": "E6VzbBl0Efsj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 7s 22ms/step - loss: 0.0769 - accuracy: 0.9861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[1]"
      ],
      "metadata": {
        "id": "rNFr6WX2EfvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eaf6554-96b8-42cb-f607-13ee21be5d3b"
      },
      "id": "rNFr6WX2EfvH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9861000180244446"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ravel.shape"
      ],
      "metadata": {
        "id": "3uTEwbw0Efxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700d9d8a-7e77-4da6-b176-fee4c4ff63ef"
      },
      "id": "3uTEwbw0Efxb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 16, 49)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 3 has an accuracy of 98.6% which is lower"
      ],
      "metadata": {
        "id": "O-zqaEntGzgl"
      },
      "id": "O-zqaEntGzgl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT Model 4:"
      ],
      "metadata": {
        "id": "N6qO9KtvGDV4"
      },
      "id": "N6qO9KtvGDV4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing the Architecture from ViT 1 to ViT 4: Here we are again only going to make a slight change. Since we saw that the accuracy drops slightly with the dropout rate going from 0.08 to 0.01 (Model 2 to Model 3), we are going to increase this dropout rate back to 0.08 and then return to the structure of Model 1. In essence, Model 4 is Model 1 with a higher dropout rate."
      ],
      "metadata": {
        "id": "YOQnlc3WGMqa"
      },
      "id": "YOQnlc3WGMqa"
    },
    {
      "cell_type": "code",
      "source": [
        "# this is written as a tensorflow \"layer\".  it's just a vector the same size as the\n",
        "# output of the previous layer. the vector is initialized randomly, but we'll use\n",
        "# gradient descent to update the values in the vector\n",
        "#\n",
        "# it's purpose is to be appended to the beginning of the sequence of vectors fed into\n",
        "# the transformer.  then after the transformer runs on the whole data, we just grab\n",
        "# the resulting zero-th vector...the class token...and use that as the portfolio weights\n",
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls"
      ],
      "metadata": {
        "id": "taml8PW5Efzz"
      },
      "id": "taml8PW5Efzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ViT4(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes):\n",
        "    # n is number of rows of blocks\n",
        "    # m is number of cols of blocks\n",
        "    # block_size is number of pixels (with rgb) in each block\n",
        "\n",
        "    inp = tf.keras.layers.Input(shape=(n*m,block_size))\n",
        "    inp2 = tf.keras.layers.Input(shape=(n*m))\n",
        "    mid = tf.keras.layers.Dense(hidden_dim)(inp) # transform to vectors with different dimension\n",
        "    # the positional embeddings\n",
        "#     positions = tf.range(start=0, limit=n*m, delta=1)\n",
        "    emb = tf.keras.layers.Embedding(input_dim=n*m, output_dim=hidden_dim)(inp2) # learned positional embedding for each of the n*m possible possitions\n",
        "    mid = mid + emb # for some reason, tf.keras.layers.Add causes an error, but + doesn't?\n",
        "    # create and append class token to beginning of all input vectors\n",
        "    token = ClassToken()(mid) # append class token to beginning of sequence\n",
        "    mid = tf.keras.layers.Concatenate(axis=1)([token, mid])\n",
        "\n",
        "    for l in range(num_layers): # how many Transformer Head layers are there?\n",
        "        ln  = tf.keras.layers.LayerNormalization()(mid) # normalize\n",
        "        mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,value_dim=key_dim)(ln,ln,ln) # self attention!\n",
        "        add = tf.keras.layers.Add()([mid,mha]) # add and norm\n",
        "        ln  = tf.keras.layers.LayerNormalization()(add)\n",
        "        den = tf.keras.layers.Dense(mlp_dim,activation='gelu')(ln) # maybe should be relu...who knows...\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den) # regularization\n",
        "        den = tf.keras.layers.Dense(hidden_dim)(den) # back to the right dimensional space\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        mid = tf.keras.layers.Add()([den,add]) # add and norm again\n",
        "    ln = tf.keras.layers.LayerNormalization()(mid)\n",
        "    fl = ln[:,0,:] # just grab the class token for each image in batch\n",
        "    clas = tf.keras.layers.Dense(num_classes,activation='softmax')(fl) # probability that the image is in each category\n",
        "    mod = tf.keras.models.Model([inp,inp2],clas)\n",
        "    mod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mod"
      ],
      "metadata": {
        "id": "uUTCFcXaEf3A"
      },
      "id": "uUTCFcXaEf3A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "m = 4\n",
        "block_size = 49\n",
        "hidden_dim = 128\n",
        "num_layers = 16\n",
        "num_heads = 8\n",
        "key_dim = hidden_dim//num_heads # usually good practice for key_dim to be hidden_dim//num_heads...this is why we do Multi-Head attention\n",
        "mlp_dim = hidden_dim\n",
        "dropout_rate = 0.08\n",
        "num_classes = 10\n",
        "\n",
        "trans = build_ViT4(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
        "trans.summary()"
      ],
      "metadata": {
        "id": "ar0y9f0gEf6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf62bbc3-b9bf-408a-8352-c6ae08725d65"
      },
      "id": "ar0y9f0gEf6T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 16, 49)]             0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 16)]                 0         []                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 16, 128)              6400      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 16, 128)              2048      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 16, 128)              0         ['dense[0][0]',               \n",
            " Lambda)                                                             'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " class_token (ClassToken)    (None, 1, 128)               128       ['tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 17, 128)              0         ['class_token[0][0]',         \n",
            "                                                                     'tf.__operators__.add[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 17, 128)              256       ['concatenate[0][0]']         \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 17, 128)              66048     ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]', \n",
            "                                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 17, 128)              0         ['concatenate[0][0]',         \n",
            "                                                                     'multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 17, 128)              256       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 17, 128)              16512     ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 17, 128)              0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 17, 128)              16512     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 17, 128)              0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 17, 128)              0         ['dropout_1[0][0]',           \n",
            "                                                                     'add[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 17, 128)              256       ['add_1[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 17, 128)              66048     ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 17, 128)              0         ['add_1[0][0]',               \n",
            "                                                                     'multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 17, 128)              256       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 17, 128)              16512     ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 17, 128)              0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 17, 128)              16512     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 17, 128)              0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 17, 128)              0         ['dropout_3[0][0]',           \n",
            "                                                                     'add_2[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 17, 128)              256       ['add_3[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 17, 128)              66048     ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 17, 128)              0         ['add_3[0][0]',               \n",
            "                                                                     'multi_head_attention_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 17, 128)              256       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 17, 128)              16512     ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 17, 128)              0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 17, 128)              16512     ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 17, 128)              0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 17, 128)              0         ['dropout_5[0][0]',           \n",
            "                                                                     'add_4[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 17, 128)              256       ['add_5[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 17, 128)              66048     ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 17, 128)              0         ['add_5[0][0]',               \n",
            "                                                                     'multi_head_attention_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 17, 128)              256       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 17, 128)              16512     ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)         (None, 17, 128)              0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 17, 128)              16512     ['dropout_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 17, 128)              0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 17, 128)              0         ['dropout_7[0][0]',           \n",
            "                                                                     'add_6[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 17, 128)              256       ['add_7[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 17, 128)              66048     ['layer_normalization_8[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_8[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 17, 128)              0         ['add_7[0][0]',               \n",
            "                                                                     'multi_head_attention_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 17, 128)              256       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 17, 128)              16512     ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 17, 128)              0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 17, 128)              16512     ['dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 17, 128)              0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 17, 128)              0         ['dropout_9[0][0]',           \n",
            "                                                                     'add_8[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_10 (La  (None, 17, 128)              256       ['add_9[0][0]']               \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 17, 128)              66048     ['layer_normalization_10[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 17, 128)              0         ['add_9[0][0]',               \n",
            "                                                                     'multi_head_attention_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_11 (La  (None, 17, 128)              256       ['add_10[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 17, 128)              16512     ['layer_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 17, 128)              0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 17, 128)              16512     ['dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 17, 128)              0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 17, 128)              0         ['dropout_11[0][0]',          \n",
            "                                                                     'add_10[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_12 (La  (None, 17, 128)              256       ['add_11[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (Mu  (None, 17, 128)              66048     ['layer_normalization_12[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 17, 128)              0         ['add_11[0][0]',              \n",
            "                                                                     'multi_head_attention_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_13 (La  (None, 17, 128)              256       ['add_12[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 17, 128)              16512     ['layer_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)        (None, 17, 128)              0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 17, 128)              16512     ['dropout_12[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 17, 128)              0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 17, 128)              0         ['dropout_13[0][0]',          \n",
            "                                                                     'add_12[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_14 (La  (None, 17, 128)              256       ['add_13[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (Mu  (None, 17, 128)              66048     ['layer_normalization_14[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 17, 128)              0         ['add_13[0][0]',              \n",
            "                                                                     'multi_head_attention_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_15 (La  (None, 17, 128)              256       ['add_14[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 17, 128)              16512     ['layer_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 17, 128)              0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 17, 128)              16512     ['dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)        (None, 17, 128)              0         ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 17, 128)              0         ['dropout_15[0][0]',          \n",
            "                                                                     'add_14[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_16 (La  (None, 17, 128)              256       ['add_15[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (Mu  (None, 17, 128)              66048     ['layer_normalization_16[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_16[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_16 (Add)                (None, 17, 128)              0         ['add_15[0][0]',              \n",
            "                                                                     'multi_head_attention_8[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_17 (La  (None, 17, 128)              256       ['add_16[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 17, 128)              16512     ['layer_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)        (None, 17, 128)              0         ['dense_17[0][0]']            \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 17, 128)              16512     ['dropout_16[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)        (None, 17, 128)              0         ['dense_18[0][0]']            \n",
            "                                                                                                  \n",
            " add_17 (Add)                (None, 17, 128)              0         ['dropout_17[0][0]',          \n",
            "                                                                     'add_16[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_18 (La  (None, 17, 128)              256       ['add_17[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (Mu  (None, 17, 128)              66048     ['layer_normalization_18[0][0]\n",
            " ltiHeadAttention)                                                  ',                            \n",
            "                                                                     'layer_normalization_18[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_18 (Add)                (None, 17, 128)              0         ['add_17[0][0]',              \n",
            "                                                                     'multi_head_attention_9[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_19 (La  (None, 17, 128)              256       ['add_18[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 17, 128)              16512     ['layer_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)        (None, 17, 128)              0         ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 17, 128)              16512     ['dropout_18[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)        (None, 17, 128)              0         ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            " add_19 (Add)                (None, 17, 128)              0         ['dropout_19[0][0]',          \n",
            "                                                                     'add_18[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_20 (La  (None, 17, 128)              256       ['add_19[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (M  (None, 17, 128)              66048     ['layer_normalization_20[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_20[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_20 (Add)                (None, 17, 128)              0         ['add_19[0][0]',              \n",
            "                                                                     'multi_head_attention_10[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_21 (La  (None, 17, 128)              256       ['add_20[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, 17, 128)              16512     ['layer_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)        (None, 17, 128)              0         ['dense_21[0][0]']            \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 17, 128)              16512     ['dropout_20[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)        (None, 17, 128)              0         ['dense_22[0][0]']            \n",
            "                                                                                                  \n",
            " add_21 (Add)                (None, 17, 128)              0         ['dropout_21[0][0]',          \n",
            "                                                                     'add_20[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_22 (La  (None, 17, 128)              256       ['add_21[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (M  (None, 17, 128)              66048     ['layer_normalization_22[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_22[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_22 (Add)                (None, 17, 128)              0         ['add_21[0][0]',              \n",
            "                                                                     'multi_head_attention_11[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_23 (La  (None, 17, 128)              256       ['add_22[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, 17, 128)              16512     ['layer_normalization_23[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)        (None, 17, 128)              0         ['dense_23[0][0]']            \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, 17, 128)              16512     ['dropout_22[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)        (None, 17, 128)              0         ['dense_24[0][0]']            \n",
            "                                                                                                  \n",
            " add_23 (Add)                (None, 17, 128)              0         ['dropout_23[0][0]',          \n",
            "                                                                     'add_22[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_24 (La  (None, 17, 128)              256       ['add_23[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (M  (None, 17, 128)              66048     ['layer_normalization_24[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_24[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_24 (Add)                (None, 17, 128)              0         ['add_23[0][0]',              \n",
            "                                                                     'multi_head_attention_12[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_25 (La  (None, 17, 128)              256       ['add_24[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_25 (Dense)            (None, 17, 128)              16512     ['layer_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)        (None, 17, 128)              0         ['dense_25[0][0]']            \n",
            "                                                                                                  \n",
            " dense_26 (Dense)            (None, 17, 128)              16512     ['dropout_24[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)        (None, 17, 128)              0         ['dense_26[0][0]']            \n",
            "                                                                                                  \n",
            " add_25 (Add)                (None, 17, 128)              0         ['dropout_25[0][0]',          \n",
            "                                                                     'add_24[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_26 (La  (None, 17, 128)              256       ['add_25[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (M  (None, 17, 128)              66048     ['layer_normalization_26[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_26[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_26 (Add)                (None, 17, 128)              0         ['add_25[0][0]',              \n",
            "                                                                     'multi_head_attention_13[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_27 (La  (None, 17, 128)              256       ['add_26[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_27 (Dense)            (None, 17, 128)              16512     ['layer_normalization_27[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)        (None, 17, 128)              0         ['dense_27[0][0]']            \n",
            "                                                                                                  \n",
            " dense_28 (Dense)            (None, 17, 128)              16512     ['dropout_26[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)        (None, 17, 128)              0         ['dense_28[0][0]']            \n",
            "                                                                                                  \n",
            " add_27 (Add)                (None, 17, 128)              0         ['dropout_27[0][0]',          \n",
            "                                                                     'add_26[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_28 (La  (None, 17, 128)              256       ['add_27[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (M  (None, 17, 128)              66048     ['layer_normalization_28[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_28[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_28 (Add)                (None, 17, 128)              0         ['add_27[0][0]',              \n",
            "                                                                     'multi_head_attention_14[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_29 (La  (None, 17, 128)              256       ['add_28[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_29 (Dense)            (None, 17, 128)              16512     ['layer_normalization_29[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)        (None, 17, 128)              0         ['dense_29[0][0]']            \n",
            "                                                                                                  \n",
            " dense_30 (Dense)            (None, 17, 128)              16512     ['dropout_28[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)        (None, 17, 128)              0         ['dense_30[0][0]']            \n",
            "                                                                                                  \n",
            " add_29 (Add)                (None, 17, 128)              0         ['dropout_29[0][0]',          \n",
            "                                                                     'add_28[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_30 (La  (None, 17, 128)              256       ['add_29[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (M  (None, 17, 128)              66048     ['layer_normalization_30[0][0]\n",
            " ultiHeadAttention)                                                 ',                            \n",
            "                                                                     'layer_normalization_30[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'layer_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " add_30 (Add)                (None, 17, 128)              0         ['add_29[0][0]',              \n",
            "                                                                     'multi_head_attention_15[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_31 (La  (None, 17, 128)              256       ['add_30[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_31 (Dense)            (None, 17, 128)              16512     ['layer_normalization_31[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)        (None, 17, 128)              0         ['dense_31[0][0]']            \n",
            "                                                                                                  \n",
            " dense_32 (Dense)            (None, 17, 128)              16512     ['dropout_30[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)        (None, 17, 128)              0         ['dense_32[0][0]']            \n",
            "                                                                                                  \n",
            " add_31 (Add)                (None, 17, 128)              0         ['dropout_31[0][0]',          \n",
            "                                                                     'add_30[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_32 (La  (None, 17, 128)              256       ['add_31[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 128)                  0         ['layer_normalization_32[0][0]\n",
            " SlicingOpLambda)                                                   ']                            \n",
            "                                                                                                  \n",
            " dense_33 (Dense)            (None, 10)                   1290      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1603466 (6.12 MB)\n",
            "Trainable params: 1603466 (6.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "ndata_train = x_train.shape[0]\n",
        "ndata_test = x_test.shape[0]"
      ],
      "metadata": {
        "id": "B-p-3d2tuG_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d49b4e-be3e-4125-942e-eec70ce1bf79"
      },
      "id": "B-p-3d2tuG_T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ravel = np.zeros((ndata_train,n*m,block_size))\n",
        "for img in range(ndata_train):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_train_ravel[img,ind,:] = x_train[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1"
      ],
      "metadata": {
        "id": "oi5a9qNREf8E"
      },
      "id": "oi5a9qNREf8E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_ravel = np.zeros((ndata_test,n*m,block_size))\n",
        "for img in range(ndata_test):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            x_test_ravel[img,ind,:] = x_test[img,(row*7):((row+1)*7),(col*7):((col+1)*7)].ravel()\n",
        "            ind += 1"
      ],
      "metadata": {
        "id": "8MqawR3tEf_5"
      },
      "id": "8MqawR3tEf_5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
        "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
      ],
      "metadata": {
        "id": "_mrsQjEpu9oS"
      },
      "id": "_mrsQjEpu9oS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your learning rate scheduler function\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr  # Keep the initial learning rate for the first 10 epochs\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)  # Exponentially decay the learning rate after 10 epochs\n",
        "\n",
        "# Create the learning rate scheduler callback\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# Define the model checkpoint callback to save the best model weights\n",
        "checkpoint_filepath = 'model_checkpoint.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Call trans.fit() with the learning rate scheduler and model checkpoint callbacks\n",
        "trans.fit([x_train_ravel, pos_feed_train], y_train,\n",
        "          epochs=100, batch_size=256, validation_split=0.20,\n",
        "          callbacks=[lr_callback, model_checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqXGa3F2vD0K",
        "outputId": "0788d33f-5e41-4a65-ecc5-fe804639b2e9"
      },
      "id": "RqXGa3F2vD0K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "188/188 [==============================] - 69s 125ms/step - loss: 0.8511 - accuracy: 0.7139 - val_loss: 0.2404 - val_accuracy: 0.9254 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 21s 112ms/step - loss: 0.2161 - accuracy: 0.9328 - val_loss: 0.1842 - val_accuracy: 0.9457 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 22s 115ms/step - loss: 0.1362 - accuracy: 0.9582 - val_loss: 0.1025 - val_accuracy: 0.9682 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 0.1046 - accuracy: 0.9672 - val_loss: 0.0960 - val_accuracy: 0.9701 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 0.0833 - accuracy: 0.9739 - val_loss: 0.1211 - val_accuracy: 0.9636 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 0.0725 - accuracy: 0.9769 - val_loss: 0.0823 - val_accuracy: 0.9753 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "188/188 [==============================] - 20s 109ms/step - loss: 0.0666 - accuracy: 0.9793 - val_loss: 0.0802 - val_accuracy: 0.9772 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 0.0641 - accuracy: 0.9792 - val_loss: 0.0804 - val_accuracy: 0.9749 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 0.0566 - accuracy: 0.9819 - val_loss: 0.1190 - val_accuracy: 0.9660 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 0.0520 - accuracy: 0.9834 - val_loss: 0.0920 - val_accuracy: 0.9743 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 0.0446 - accuracy: 0.9859 - val_loss: 0.0716 - val_accuracy: 0.9791 - lr: 9.0484e-04\n",
            "Epoch 12/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 0.0338 - accuracy: 0.9884 - val_loss: 0.0759 - val_accuracy: 0.9775 - lr: 8.1873e-04\n",
            "Epoch 13/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.0667 - val_accuracy: 0.9828 - lr: 7.4082e-04\n",
            "Epoch 14/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 0.0274 - accuracy: 0.9904 - val_loss: 0.0736 - val_accuracy: 0.9791 - lr: 6.7032e-04\n",
            "Epoch 15/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 0.0219 - accuracy: 0.9929 - val_loss: 0.0718 - val_accuracy: 0.9820 - lr: 6.0653e-04\n",
            "Epoch 16/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 0.0171 - accuracy: 0.9940 - val_loss: 0.0603 - val_accuracy: 0.9843 - lr: 5.4881e-04\n",
            "Epoch 17/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0634 - val_accuracy: 0.9838 - lr: 4.9659e-04\n",
            "Epoch 18/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0693 - val_accuracy: 0.9842 - lr: 4.4933e-04\n",
            "Epoch 19/100\n",
            "188/188 [==============================] - 21s 113ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0668 - val_accuracy: 0.9855 - lr: 4.0657e-04\n",
            "Epoch 20/100\n",
            "188/188 [==============================] - 22s 115ms/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 0.0687 - val_accuracy: 0.9833 - lr: 3.6788e-04\n",
            "Epoch 21/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 0.0742 - val_accuracy: 0.9827 - lr: 3.3287e-04\n",
            "Epoch 22/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0729 - val_accuracy: 0.9841 - lr: 3.0119e-04\n",
            "Epoch 23/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.0695 - val_accuracy: 0.9864 - lr: 2.7253e-04\n",
            "Epoch 24/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0714 - val_accuracy: 0.9858 - lr: 2.4660e-04\n",
            "Epoch 25/100\n",
            "188/188 [==============================] - 19s 104ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0695 - val_accuracy: 0.9852 - lr: 2.2313e-04\n",
            "Epoch 26/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0654 - val_accuracy: 0.9863 - lr: 2.0190e-04\n",
            "Epoch 27/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0703 - val_accuracy: 0.9852 - lr: 1.8268e-04\n",
            "Epoch 28/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0785 - val_accuracy: 0.9847 - lr: 1.6530e-04\n",
            "Epoch 29/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0696 - val_accuracy: 0.9864 - lr: 1.4957e-04\n",
            "Epoch 30/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0700 - val_accuracy: 0.9859 - lr: 1.3534e-04\n",
            "Epoch 31/100\n",
            "188/188 [==============================] - 20s 109ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0703 - val_accuracy: 0.9866 - lr: 1.2246e-04\n",
            "Epoch 32/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 8.0507e-04 - accuracy: 0.9999 - val_loss: 0.0694 - val_accuracy: 0.9865 - lr: 1.1080e-04\n",
            "Epoch 33/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 8.4919e-04 - accuracy: 0.9998 - val_loss: 0.0697 - val_accuracy: 0.9867 - lr: 1.0026e-04\n",
            "Epoch 34/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 7.7516e-04 - accuracy: 0.9998 - val_loss: 0.0727 - val_accuracy: 0.9866 - lr: 9.0718e-05\n",
            "Epoch 35/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 5.6422e-04 - accuracy: 0.9999 - val_loss: 0.0711 - val_accuracy: 0.9870 - lr: 8.2085e-05\n",
            "Epoch 36/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 5.3569e-04 - accuracy: 0.9999 - val_loss: 0.0711 - val_accuracy: 0.9864 - lr: 7.4273e-05\n",
            "Epoch 37/100\n",
            "188/188 [==============================] - 20s 109ms/step - loss: 5.4011e-04 - accuracy: 0.9999 - val_loss: 0.0714 - val_accuracy: 0.9865 - lr: 6.7205e-05\n",
            "Epoch 38/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 6.6167e-04 - accuracy: 0.9999 - val_loss: 0.0739 - val_accuracy: 0.9863 - lr: 6.0810e-05\n",
            "Epoch 39/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 4.5078e-04 - accuracy: 0.9999 - val_loss: 0.0728 - val_accuracy: 0.9868 - lr: 5.5023e-05\n",
            "Epoch 40/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 4.1460e-04 - accuracy: 0.9999 - val_loss: 0.0736 - val_accuracy: 0.9865 - lr: 4.9787e-05\n",
            "Epoch 41/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 4.6542e-04 - accuracy: 0.9999 - val_loss: 0.0744 - val_accuracy: 0.9865 - lr: 4.5049e-05\n",
            "Epoch 42/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 5.3684e-04 - accuracy: 0.9999 - val_loss: 0.0733 - val_accuracy: 0.9872 - lr: 4.0762e-05\n",
            "Epoch 43/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 6.7248e-04 - accuracy: 0.9998 - val_loss: 0.0714 - val_accuracy: 0.9875 - lr: 3.6883e-05\n",
            "Epoch 44/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 4.2861e-04 - accuracy: 0.9999 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 3.3373e-05\n",
            "Epoch 45/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 6.4154e-04 - accuracy: 0.9998 - val_loss: 0.0728 - val_accuracy: 0.9867 - lr: 3.0197e-05\n",
            "Epoch 46/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 4.1061e-04 - accuracy: 0.9999 - val_loss: 0.0740 - val_accuracy: 0.9870 - lr: 2.7324e-05\n",
            "Epoch 47/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 3.6069e-04 - accuracy: 0.9999 - val_loss: 0.0724 - val_accuracy: 0.9873 - lr: 2.4723e-05\n",
            "Epoch 48/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 3.0875e-04 - accuracy: 0.9999 - val_loss: 0.0757 - val_accuracy: 0.9870 - lr: 2.2371e-05\n",
            "Epoch 49/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 3.2574e-04 - accuracy: 0.9999 - val_loss: 0.0754 - val_accuracy: 0.9872 - lr: 2.0242e-05\n",
            "Epoch 50/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 2.6121e-04 - accuracy: 0.9999 - val_loss: 0.0746 - val_accuracy: 0.9872 - lr: 1.8316e-05\n",
            "Epoch 51/100\n",
            "188/188 [==============================] - 19s 103ms/step - loss: 2.5397e-04 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9874 - lr: 1.6573e-05\n",
            "Epoch 52/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 4.6301e-04 - accuracy: 0.9999 - val_loss: 0.0731 - val_accuracy: 0.9872 - lr: 1.4996e-05\n",
            "Epoch 53/100\n",
            "188/188 [==============================] - 21s 112ms/step - loss: 4.0075e-04 - accuracy: 0.9999 - val_loss: 0.0744 - val_accuracy: 0.9875 - lr: 1.3569e-05\n",
            "Epoch 54/100\n",
            "188/188 [==============================] - 19s 104ms/step - loss: 2.8422e-04 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9871 - lr: 1.2277e-05\n",
            "Epoch 55/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 3.5925e-04 - accuracy: 0.9999 - val_loss: 0.0738 - val_accuracy: 0.9870 - lr: 1.1109e-05\n",
            "Epoch 56/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 2.0391e-04 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9868 - lr: 1.0052e-05\n",
            "Epoch 57/100\n",
            "188/188 [==============================] - 19s 102ms/step - loss: 2.7341e-04 - accuracy: 0.9999 - val_loss: 0.0743 - val_accuracy: 0.9869 - lr: 9.0953e-06\n",
            "Epoch 58/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 3.6511e-04 - accuracy: 0.9999 - val_loss: 0.0744 - val_accuracy: 0.9871 - lr: 8.2297e-06\n",
            "Epoch 59/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 2.9737e-04 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9872 - lr: 7.4466e-06\n",
            "Epoch 60/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 1.8246e-04 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9872 - lr: 6.7379e-06\n",
            "Epoch 61/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 3.4928e-04 - accuracy: 0.9999 - val_loss: 0.0734 - val_accuracy: 0.9872 - lr: 6.0967e-06\n",
            "Epoch 62/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 1.9474e-04 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9872 - lr: 5.5165e-06\n",
            "Epoch 63/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.8442e-04 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9872 - lr: 4.9916e-06\n",
            "Epoch 64/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 1.5357e-04 - accuracy: 1.0000 - val_loss: 0.0731 - val_accuracy: 0.9873 - lr: 4.5166e-06\n",
            "Epoch 65/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.5794e-04 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 0.9873 - lr: 4.0868e-06\n",
            "Epoch 66/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 2.8216e-04 - accuracy: 0.9999 - val_loss: 0.0733 - val_accuracy: 0.9876 - lr: 3.6979e-06\n",
            "Epoch 67/100\n",
            "188/188 [==============================] - 20s 104ms/step - loss: 2.0342e-04 - accuracy: 0.9999 - val_loss: 0.0737 - val_accuracy: 0.9875 - lr: 3.3460e-06\n",
            "Epoch 68/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 2.4464e-04 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9874 - lr: 3.0275e-06\n",
            "Epoch 69/100\n",
            "188/188 [==============================] - 20s 109ms/step - loss: 1.7108e-04 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9874 - lr: 2.7394e-06\n",
            "Epoch 70/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.2930e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9874 - lr: 2.4787e-06\n",
            "Epoch 71/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 1.4784e-04 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9875 - lr: 2.2429e-06\n",
            "Epoch 72/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 1.5943e-04 - accuracy: 1.0000 - val_loss: 0.0733 - val_accuracy: 0.9877 - lr: 2.0294e-06\n",
            "Epoch 73/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 1.4615e-04 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9878 - lr: 1.8363e-06\n",
            "Epoch 74/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.4561e-04 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9876 - lr: 1.6616e-06\n",
            "Epoch 75/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 1.7236e-04 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9877 - lr: 1.5034e-06\n",
            "Epoch 76/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 2.5603e-04 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 0.9877 - lr: 1.3604e-06\n",
            "Epoch 77/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.7354e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9877 - lr: 1.2309e-06\n",
            "Epoch 78/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 2.0691e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9877 - lr: 1.1138e-06\n",
            "Epoch 79/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.6850e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9877 - lr: 1.0078e-06\n",
            "Epoch 80/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 2.7409e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9875 - lr: 9.1188e-07\n",
            "Epoch 81/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 2.3575e-04 - accuracy: 0.9999 - val_loss: 0.0740 - val_accuracy: 0.9876 - lr: 8.2510e-07\n",
            "Epoch 82/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.2775e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9876 - lr: 7.4658e-07\n",
            "Epoch 83/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 1.8173e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 6.7554e-07\n",
            "Epoch 84/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 2.2983e-04 - accuracy: 0.9999 - val_loss: 0.0738 - val_accuracy: 0.9874 - lr: 6.1125e-07\n",
            "Epoch 85/100\n",
            "188/188 [==============================] - 21s 112ms/step - loss: 2.2266e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 5.5308e-07\n",
            "Epoch 86/100\n",
            "188/188 [==============================] - 22s 115ms/step - loss: 2.8351e-04 - accuracy: 0.9999 - val_loss: 0.0739 - val_accuracy: 0.9875 - lr: 5.0045e-07\n",
            "Epoch 87/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 3.1140e-04 - accuracy: 0.9999 - val_loss: 0.0739 - val_accuracy: 0.9875 - lr: 4.5283e-07\n",
            "Epoch 88/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 1.4745e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 4.0973e-07\n",
            "Epoch 89/100\n",
            "188/188 [==============================] - 21s 109ms/step - loss: 1.3056e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9875 - lr: 3.7074e-07\n",
            "Epoch 90/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 2.4341e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9875 - lr: 3.3546e-07\n",
            "Epoch 91/100\n",
            "188/188 [==============================] - 20s 106ms/step - loss: 1.1195e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9875 - lr: 3.0354e-07\n",
            "Epoch 92/100\n",
            "188/188 [==============================] - 21s 112ms/step - loss: 2.1009e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 2.7465e-07\n",
            "Epoch 93/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.5067e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 2.4852e-07\n",
            "Epoch 94/100\n",
            "188/188 [==============================] - 20s 108ms/step - loss: 1.6573e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 2.2487e-07\n",
            "Epoch 95/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 1.6406e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9875 - lr: 2.0347e-07\n",
            "Epoch 96/100\n",
            "188/188 [==============================] - 20s 107ms/step - loss: 1.3839e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 1.8410e-07\n",
            "Epoch 97/100\n",
            "188/188 [==============================] - 20s 105ms/step - loss: 2.5589e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 1.6659e-07\n",
            "Epoch 98/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 1.6557e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 1.5073e-07\n",
            "Epoch 99/100\n",
            "188/188 [==============================] - 21s 111ms/step - loss: 1.2010e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 1.3639e-07\n",
            "Epoch 100/100\n",
            "188/188 [==============================] - 21s 110ms/step - loss: 3.3115e-04 - accuracy: 0.9999 - val_loss: 0.0739 - val_accuracy: 0.9874 - lr: 1.2341e-07\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a5860139930>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model for reproducibility\n",
        "\n",
        "trans.save(\"/content/drive/MyDrive/transformer_5.h5\")"
      ],
      "metadata": {
        "id": "_Vp80RpTvWSu"
      },
      "id": "_Vp80RpTvWSu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = trans.evaluate([x_test_ravel,pos_feed_test],y_test)"
      ],
      "metadata": {
        "id": "3YD3opZCvU7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23655c84-5508-413b-9fad-2eae663db01e"
      },
      "id": "3YD3opZCvU7b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 11s 34ms/step - loss: 0.0749 - accuracy: 0.9863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[1]"
      ],
      "metadata": {
        "id": "5ux6Ou5zvWtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb63f336-07d4-4352-bfb6-a67d831f8c6f"
      },
      "id": "5ux6Ou5zvWtq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.986299991607666"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link to Models 5, 6, 7, 8\n",
        "\n",
        "Since the run time was taking a long time we split the models into different Colab files and ran them on different computers/ipads/etc..."
      ],
      "metadata": {
        "id": "zscBbPU-aISr"
      },
      "id": "zscBbPU-aISr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1Uls9-3iDvFdKDrF7_N_5n-pmvA6ZPiNA?usp=sharing"
      ],
      "metadata": {
        "id": "Fbbls1-2aHKs"
      },
      "id": "Fbbls1-2aHKs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link to Models 9, 10, 11, 12:"
      ],
      "metadata": {
        "id": "jU0wYhFiilI-"
      },
      "id": "jU0wYhFiilI-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1qRwrLwL2xPWxinZGY-N06kE7VEBm8Ji4?usp=sharing"
      ],
      "metadata": {
        "id": "IFDZlVD0il9E"
      },
      "id": "IFDZlVD0il9E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link to Model 13:"
      ],
      "metadata": {
        "id": "bkrss-zNtMby"
      },
      "id": "bkrss-zNtMby"
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1IO3oOYNWwE73sTsgd-pcV73u3-V5sBC0?usp=sharing"
      ],
      "metadata": {
        "id": "Y84AVwV0tM0Q"
      },
      "id": "Y84AVwV0tM0Q"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
